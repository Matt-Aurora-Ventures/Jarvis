---
phase: 01-core-backtesting-infrastructure
plan: 02
type: execute
wave: 2
depends_on: ["01"]
files_modified: [core/backtest/dataset/crypto_dataset.py, core/backtest/tuner/tuning_loop.py, core/backtest/models/base_sniper.py]
autonomous: true
requirements: [py-torch-suite]

must_haves:
  truths:
    - "PyTorch datasets can ingest pandas DataFrames"
    - "PyTorch model is scafolded to emulate current heuristics"
    - "Optuna/Bayesian loops can inject variables into the backtesting evaluation"
  artifacts:
    - path: "core/backtest/dataset/crypto_dataset.py"
      provides: "PyTorch Dataset definition"
    - path: "core/backtest/tuner/tuning_loop.py"
      provides: "Optuna tuner loop"
  key_links: []
---

<objective>
Implement the PyTorch Dataset logic and the Optuna optimization loop.

Purpose: To execute massive, parallelized backtesting logic to evaluate trading signals, we need to structure data correctly and wrap the execution in a tuning loop.
Output: Core classes representing the `CryptoTimeSeriesDataset` and the `HyperParameterTuner`.
</objective>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-core-backtesting-infrastructure/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Scaffold PyTorch Dataset</name>
  <files>core/backtest/dataset/crypto_dataset.py</files>
  <action>
    Implement `CryptoTimeSeriesDataset(torch.utils.data.Dataset)`.
    It should take a pandas DataFrame (result of `fetch_historical_data`) and window length, returning sequences of sequences `(X, y)` suitable for PyTorch ingestion.
    Implement necessary scaling logic (e.g. MinMax or Standard scaling wrappers).
  </action>
  <verify>Run the module locally with mock data and verify the `shape` of batches from `DataLoader`.</verify>
  <done>Dataset class functions successfully on OHLCV data.</done>
</task>

<task type="auto">
  <name>Task 2: Define Base PyTorch Sniper Model</name>
  <files>core/backtest/models/base_sniper.py</files>
  <action>
    Implement an initial PyTorch `nn.Module` (e.g., `LSTM` or simple FeedForward connected to the strategy output heuristics).
    This acts as the bridge that takes parameter inputs and creates a predictive output score for our sniper strategies (like `risk_tier` bounding, etc).
  </action>
  <verify>Instantiate the model and pass a random tensor simulating a batch query, avoiding runtime errors.</verify>
  <done>Model definition loads without syntactical errors.</done>
</task>

<task type="auto">
  <name>Task 3: Implement Optuna Optimization Loop</name>
  <files>core/backtest/tuner/tuning_loop.py</files>
  <action>
    Implement the Optuna wrapper `run_optimization_study()`.
    Define an objective function that:
      1) suggests hyperparameters (e.g. SL percentage, TP percentage, trailing step),
      2) runs a simulated execution over the PyTorch dataset,
      3) returns the simulated portfolio P/L (Profit & Loss) metric.
  </action>
  <verify>Write a test script to execute a quick 2-3 trial Optuna loop using random mock returns.</verify>
  <done>Optuna loop runs, optimizes parameters, and outputs the "best" trial metrics successfully.</done>
</task>

</tasks>

<verification>
- [ ] PyTorch Dataset returns correct dimensions.
- [ ] Optuna can instantiate a study and simulate iterations.
- [ ] Code has typing defined.
</verification>

<success_criteria>
- The pipeline functions effectively allowing `data -> dataset -> model + parameters -> optimization eval`.
- Modular structure ensures the actual live bots can utilize these trained bounds.
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-backtesting-infrastructure/01-02-SUMMARY.md`
</output>
