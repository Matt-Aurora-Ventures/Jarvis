---
phase: 07-retain-recall-functions
plan: 06
type: execute
wave: 4
depends_on: ["07-03", "07-04", "07-05"]
files_modified:
  - tests/integration/test_memory_integration.py
  - tests/integration/test_memory_performance.py
autonomous: true

must_haves:
  truths:
    - "All 5 bots can write to memory without errors"
    - "Recall queries complete in <100ms at p95"
    - "5 bots can write concurrently without conflicts"
    - "Entity extraction correctly identifies @tokens, @users, @strategies"
    - "Preference confidence evolves with evidence (confirm/contradict)"
  artifacts:
    - path: "tests/integration/test_memory_integration.py"
      provides: "Integration tests for all bot memory hooks"
      min_lines: 200
    - path: "tests/integration/test_memory_performance.py"
      provides: "Performance benchmarks and concurrent access tests"
      min_lines: 100
  key_links:
    - from: "tests/integration/test_memory_integration.py"
      to: "bots/*/memory_hooks.py"
      via: "import and test each bot's memory hooks"
      pattern: "from bots\\.(treasury|twitter|bags_intel|buy_tracker)\\.memory_hooks import"
---

<objective>
Create comprehensive integration tests and performance validation for the memory system across all 5 bots.

Purpose: Verify that the memory integration works correctly under realistic conditions - multiple bots writing concurrently, recall queries meeting latency targets, entity extraction accuracy, and preference confidence evolution.

Output:
- tests/integration/test_memory_integration.py with bot integration tests
- tests/integration/test_memory_performance.py with benchmarks and concurrent tests
</objective>

<execution_context>
@C:\Users\lucid\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\lucid\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/memory-integration/ROADMAP.md
@.planning/memory-integration/phases/07-retain-recall-functions/07-RESEARCH.md

# Phase 7 Wave 2-3 outputs
@bots/treasury/trading/memory_hooks.py
@tg_bot/services/memory_service.py
@bots/twitter/memory_hooks.py
@bots/bags_intel/memory_hooks.py
@bots/buy_tracker/memory_hooks.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create bot integration tests</name>
  <files>tests/integration/test_memory_integration.py</files>
  <action>
    Create tests/integration/test_memory_integration.py with comprehensive bot tests:

    1. Add test infrastructure:
       ```python
       import pytest
       import asyncio
       from pathlib import Path
       import tempfile
       import os

       # Set test memory path
       @pytest.fixture(autouse=True)
       def setup_test_memory(tmp_path):
           os.environ["JARVIS_MEMORY_ROOT"] = str(tmp_path / "memory")
           from core.memory import init_workspace
           init_workspace()
           yield
           # Cleanup handled by tmp_path fixture
       ```

    2. Test Treasury memory hooks:
       ```python
       class TestTreasuryMemory:
           @pytest.mark.asyncio
           async def test_store_trade_outcome(self):
               """Trade outcomes are stored correctly"""

           @pytest.mark.asyncio
           async def test_recall_token_history(self):
               """Past trades can be recalled by token"""

           @pytest.mark.asyncio
           async def test_should_enter_based_on_history(self):
               """History informs entry decisions"""

           @pytest.mark.asyncio
           async def test_strategy_performance_tracking(self):
               """Strategy performance is tracked across trades"""
       ```

    3. Test Telegram memory hooks:
       ```python
       class TestTelegramMemory:
           @pytest.mark.asyncio
           async def test_preference_detection(self):
               """Preferences are detected from messages"""

           @pytest.mark.asyncio
           async def test_preference_storage(self):
               """Preferences are stored with confidence"""

           @pytest.mark.asyncio
           async def test_preference_confidence_evolution(self):
               """Confidence increases with confirmations"""

           @pytest.mark.asyncio
           async def test_user_context_retrieval(self):
               """Full user context can be retrieved"""

           @pytest.mark.asyncio
           async def test_response_personalization(self):
               """Responses are personalized based on preferences"""
       ```

    4. Test X/Twitter memory hooks:
       ```python
       class TestTwitterMemory:
           @pytest.mark.asyncio
           async def test_store_post_performance(self):
               """Post metrics are stored"""

           @pytest.mark.asyncio
           async def test_recall_engagement_patterns(self):
               """High-engagement posts can be recalled"""
       ```

    5. Test Bags Intel memory hooks:
       ```python
       class TestBagsIntelMemory:
           @pytest.mark.asyncio
           async def test_store_graduation_outcome(self):
               """Graduation outcomes are stored"""

           @pytest.mark.asyncio
           async def test_recall_similar_graduations(self):
               """Similar graduations can be recalled"""

           @pytest.mark.asyncio
           async def test_graduation_success_prediction(self):
               """Success probability based on history"""
       ```

    6. Test Buy Tracker memory hooks:
       ```python
       class TestBuyTrackerMemory:
           @pytest.mark.asyncio
           async def test_store_purchase_event(self):
               """Purchase events are stored"""

           @pytest.mark.asyncio
           async def test_recall_purchase_history(self):
               """Purchase history can be recalled"""
       ```

    7. Test entity extraction:
       ```python
       class TestEntityExtraction:
           def test_token_extraction(self):
               """@TOKEN and $TOKEN extracted correctly"""

           def test_user_extraction(self):
               """@username extracted correctly"""

           def test_strategy_extraction(self):
               """Strategy names extracted from context"""

           def test_entity_linking(self):
               """Entities linked to facts correctly"""
       ```

    8. Test session context:
       ```python
       class TestSessionContext:
           def test_save_and_retrieve_session(self):
               """Session context persists"""

           def test_cross_restart_persistence(self):
               """Context survives session restart"""
       ```

    Each test should:
    - Have clear docstring
    - Test specific functionality
    - Assert expected outcomes
    - Clean up test data

    Do NOT skip any bot - all 5 must be tested.
    Do NOT use mocks for core memory functions - test real integration.
  </action>
  <verify>
    ```bash
    cd "c:\Users\lucid\OneDrive\Desktop\Projects\Jarvis"
    python -m pytest tests/integration/test_memory_integration.py -v --tb=short 2>&1 | head -50
    ```
  </verify>
  <done>Integration tests cover all 5 bots, entity extraction, session context, and preference evolution</done>
</task>

<task type="auto">
  <name>Task 2: Create performance benchmarks</name>
  <files>tests/integration/test_memory_performance.py</files>
  <action>
    Create tests/integration/test_memory_performance.py with performance tests:

    1. Add benchmark infrastructure:
       ```python
       import pytest
       import asyncio
       import time
       import statistics
       from concurrent.futures import ThreadPoolExecutor
       import threading
       ```

    2. Test recall latency:
       ```python
       class TestRecallPerformance:
           @pytest.mark.asyncio
           async def test_recall_latency_p95_under_100ms(self):
               """
               PERF-001: Recall queries complete in <100ms at p95

               1. Populate database with 1000 facts
               2. Run 100 recall queries
               3. Assert p95 latency < 100ms
               """
               from core.memory import retain_fact, recall

               # Populate
               for i in range(1000):
                   await retain_fact(
                       content=f"Test fact {i} about trading and tokens",
                       context=f"test_context|{i}",
                       entities=[f"@TOKEN{i % 10}"],
                       source="performance_test"
                   )

               # Benchmark
               latencies = []
               for i in range(100):
                   start = time.perf_counter()
                   await recall(f"trading token{i % 10}", k=10)
                   latencies.append((time.perf_counter() - start) * 1000)

               p95 = sorted(latencies)[94]  # 95th percentile
               assert p95 < 100, f"p95 latency {p95:.2f}ms exceeds 100ms target"
               print(f"Recall latency: p50={statistics.median(latencies):.2f}ms, p95={p95:.2f}ms")
       ```

    3. Test concurrent access:
       ```python
       class TestConcurrentAccess:
           @pytest.mark.asyncio
           async def test_5_bots_concurrent_writes(self):
               """
               PERF-004: 5 bots can write concurrently without conflicts

               Simulate 5 bots writing facts simultaneously.
               Assert no errors and all facts stored.
               """
               from core.memory import retain_fact
               import asyncio

               async def bot_write(bot_name: str, n_facts: int):
                   for i in range(n_facts):
                       await retain_fact(
                           content=f"{bot_name} fact {i}",
                           context=f"{bot_name}_context",
                           source=bot_name
                       )
                   return n_facts

               bots = ["treasury", "telegram", "twitter", "bags_intel", "buy_tracker"]
               results = await asyncio.gather(*[
                   bot_write(bot, 20) for bot in bots
               ])

               assert sum(results) == 100
               print(f"5 bots wrote {sum(results)} facts concurrently")

           @pytest.mark.asyncio
           async def test_concurrent_read_write(self):
               """
               PERF-004: Read and write operations can happen concurrently

               Simulate mixed read/write workload.
               """
               from core.memory import retain_fact, recall
               import asyncio

               async def writer():
                   for i in range(50):
                       await retain_fact(
                           content=f"Writer fact {i}",
                           context="concurrent_test",
                           source="writer"
                       )
                   return "write_done"

               async def reader():
                   results = []
                   for i in range(50):
                       r = await recall("writer fact", k=5)
                       results.append(len(r))
                   return results

               write_task = asyncio.create_task(writer())
               read_task = asyncio.create_task(reader())

               await asyncio.gather(write_task, read_task)
               print("Concurrent read/write completed without errors")
       ```

    4. Test entity extraction accuracy:
       ```python
       class TestEntityAccuracy:
           def test_entity_extraction_accuracy(self):
               """
               QUAL-003: Entity mentions correctly extracted

               Test accuracy on sample data.
               """
               from core.memory.markdown_sync import extract_entities_from_text

               test_cases = [
                   ("Bought @KR8TIV via momentum strategy", ["@KR8TIV", "momentum"]),
                   ("$SOL pump on bags.fm", ["SOL"]),
                   ("Trading BONK with breakout strategy", ["BONK", "breakout"]),
               ]

               correct = 0
               for text, expected in test_cases:
                   extracted = extract_entities_from_text(text)
                   # Check if expected entities are found
                   if all(e in extracted for e in expected):
                       correct += 1

               accuracy = correct / len(test_cases)
               assert accuracy >= 0.85, f"Entity accuracy {accuracy:.1%} below 85% target"
               print(f"Entity extraction accuracy: {accuracy:.1%}")
       ```

    5. Test preference confidence evolution:
       ```python
       class TestPreferenceEvolution:
           @pytest.mark.asyncio
           async def test_confidence_increases_with_confirmations(self):
               """
               QUAL-002: Preference confidence evolves with evidence

               Confirm same preference 3 times, confidence should increase.
               """
               from core.memory import retain_preference, get_user_preferences

               user_id = "test_user_123"

               # Initial preference (confidence 0.5)
               await retain_preference(
                   user=user_id,
                   key="risk_tolerance",
                   value="high",
                   evidence="First mention"
               )
               prefs = await get_user_preferences(user_id)
               initial_conf = prefs.get("risk_tolerance", {}).get("confidence", 0.5)

               # Confirm twice more
               for i in range(2):
                   await retain_preference(
                       user=user_id,
                       key="risk_tolerance",
                       value="high",
                       evidence=f"Confirmation {i+1}"
                   )

               prefs = await get_user_preferences(user_id)
               final_conf = prefs.get("risk_tolerance", {}).get("confidence", 0.5)

               assert final_conf > initial_conf, "Confidence should increase with confirmations"
               print(f"Confidence evolved: {initial_conf:.2f} -> {final_conf:.2f}")

           @pytest.mark.asyncio
           async def test_confidence_decreases_with_contradictions(self):
               """
               QUAL-002: Contradictions decrease confidence

               Set preference then contradict it, confidence should decrease.
               """
               from core.memory import retain_preference, get_user_preferences

               user_id = "test_user_456"

               # Initial preference
               await retain_preference(
                   user=user_id,
                   key="trading_style",
                   value="aggressive",
                   evidence="Initial"
               )

               # Contradict
               await retain_preference(
                   user=user_id,
                   key="trading_style",
                   value="conservative",  # Different value
                   evidence="Contradiction"
               )

               prefs = await get_user_preferences(user_id)
               conf = prefs.get("trading_style", {}).get("confidence", 0.5)

               # After contradiction, should be lower than initial 0.5
               # or value should have changed if confidence dropped below threshold
               print(f"After contradiction: value={prefs.get('trading_style', {}).get('value')}, confidence={conf:.2f}")
       ```

    6. Add performance summary:
       ```python
       class TestPerformanceSummary:
           @pytest.mark.asyncio
           async def test_full_performance_report(self):
               """Generate full performance report"""
               # Run all benchmarks and print summary
               print("\n=== MEMORY PERFORMANCE REPORT ===")
               print("Recall latency: PASS if p95 < 100ms")
               print("Concurrent access: PASS if no conflicts")
               print("Entity accuracy: PASS if >= 85%")
               print("Preference evolution: PASS if confidence changes")
       ```

    Do NOT skip performance assertions - they validate requirements.
    Do NOT use unrealistic data sizes - 1000 facts is realistic.
  </action>
  <verify>
    ```bash
    cd "c:\Users\lucid\OneDrive\Desktop\Projects\Jarvis"
    python -m pytest tests/integration/test_memory_performance.py -v --tb=short 2>&1 | head -50
    ```
  </verify>
  <done>Performance benchmarks validate <100ms recall, concurrent access, entity accuracy, and preference evolution</done>
</task>

<task type="auto">
  <name>Task 3: Run full test suite and document results</name>
  <files>None (verification only)</files>
  <action>
    Run the complete test suite and document results:

    1. Run integration tests:
       ```bash
       cd "c:\Users\lucid\OneDrive\Desktop\Projects\Jarvis"
       python -m pytest tests/integration/test_memory_integration.py tests/integration/test_memory_performance.py -v --tb=short
       ```

    2. Capture results and document:
       - Total tests passed/failed
       - Recall p95 latency
       - Concurrent access result
       - Entity accuracy percentage
       - Any failures or issues

    3. If any tests fail:
       - Document the failure
       - Identify root cause
       - Fix if possible in this task
       - Or document for follow-up

    4. Generate coverage report if pytest-cov available:
       ```bash
       python -m pytest tests/integration/test_memory_*.py --cov=core.memory --cov=bots --cov-report=term-missing
       ```

    5. Document final status:
       - All requirements validated
       - Performance targets met
       - Ready for Phase 8

    Do NOT skip test run - actual execution validates the integration.
    Do NOT ignore failures - document and fix or escalate.
  </action>
  <verify>
    ```bash
    cd "c:\Users\lucid\OneDrive\Desktop\Projects\Jarvis"
    python -m pytest tests/integration/test_memory_*.py --tb=short -q
    echo "Exit code: $?"
    ```
  </verify>
  <done>Full test suite passes, performance targets met, requirements validated</done>
</task>

</tasks>

<verification>
1. Run tests: `pytest tests/integration/test_memory_*.py -v`
2. All 5 bots tested: Treasury, Telegram, Twitter, Bags Intel, Buy Tracker
3. Recall p95 < 100ms
4. Concurrent 5-bot writes succeed without conflicts
5. Entity extraction accuracy >= 85%
6. Preference confidence evolution works
7. No test failures
</verification>

<success_criteria>
- Integration tests pass for all 5 bots
- Performance benchmarks pass:
  - Recall p95 < 100ms (PERF-001)
  - 5 concurrent writers work (PERF-004)
- Quality validations pass:
  - Entity extraction >= 85% accuracy (QUAL-003)
  - Preference confidence evolves (QUAL-002)
- All requirements validated
- Ready for Phase 8 (Reflect & Intelligence)
</success_criteria>

<output>
After completion, create `.planning/memory-integration/phases/07-retain-recall-functions/07-06-SUMMARY.md`
</output>
