---
phase: 06-memory-foundation
plan: 04
type: execute
wave: 2
depends_on: ["06-01", "06-02"]
files_modified:
  - core/memory/search.py
autonomous: true

must_haves:
  truths:
    - "FTS5 full-text search returns results from facts table"
    - "Search supports BM25 ranking for relevance ordering"
    - "Search completes in <100ms for typical queries"
    - "Temporal filters work (last 7 days, last 30 days, all time)"
  artifacts:
    - path: "core/memory/search.py"
      provides: "FTS5 search functions"
      exports: ["search_facts", "search_by_entity", "search_by_source"]
  key_links:
    - from: "core/memory/search.py"
      to: "facts_fts"
      via: "FTS5 MATCH query"
      pattern: "facts_fts.*MATCH"
---

<objective>
Implement FTS5 full-text search across all stored facts with BM25 ranking and temporal filtering.

Purpose: Enable fast retrieval of relevant memories before decisions. The FTS5 search is the first layer of the hybrid search system (combined with PostgreSQL vector search in Plan 05).

Output:
- core/memory/search.py with search_facts() and related functions
- <100ms query latency for typical searches
</objective>

<execution_context>
@C:\Users\lucid\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\lucid\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/memory-integration/PROJECT.md
@.planning/memory-integration/phases/06-memory-foundation/06-RESEARCH.md

From RESEARCH.md - FTS5 Query Pattern:
```python
results = conn.execute("""
    SELECT id, content, bm25(facts_fts) as score
    FROM facts_fts
    WHERE facts_fts MATCH ?
    ORDER BY bm25(facts_fts)
    LIMIT 10
""", (query,)).fetchall()
```

Performance requirement from REQUIREMENTS.md:
- PERF-001: Recall queries execute in <100ms p95
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create FTS5 search module</name>
  <files>core/memory/search.py</files>
  <action>
Create search.py with comprehensive FTS5 search functions.

```python
"""FTS5 full-text search for Jarvis memory system."""
import time
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any, Literal

from .database import get_db_manager


# Type aliases
TimeFilter = Literal["all", "today", "week", "month", "quarter", "year"]
SourceFilter = Literal["telegram", "treasury", "x", "bags_intel", "buy_tracker", "system", None]


def search_facts(
    query: str,
    limit: int = 10,
    time_filter: TimeFilter = "all",
    source: Optional[SourceFilter] = None,
    min_confidence: float = 0.0,
    include_inactive: bool = False,
) -> List[Dict[str, Any]]:
    """
    Search facts using FTS5 full-text search with BM25 ranking.

    Args:
        query: Search query (FTS5 syntax supported).
        limit: Maximum results to return (default 10).
        time_filter: Temporal filter ('all', 'today', 'week', 'month', 'quarter', 'year').
        source: Filter by source system.
        min_confidence: Minimum confidence threshold (0.0-1.0).
        include_inactive: Include soft-deleted facts.

    Returns:
        List of fact dicts with id, content, context, source, confidence, score.

    Example:
        results = search_facts("bags.fm graduation", limit=5, time_filter="week")
    """
    db = get_db_manager()
    start_time = time.perf_counter()

    # Build time filter
    time_clause, time_params = _build_time_filter(time_filter)

    # Build source filter
    source_clause = ""
    source_params: List[Any] = []
    if source:
        source_clause = "AND f.source = ?"
        source_params = [source]

    # Build active filter
    active_clause = "" if include_inactive else "AND f.is_active = 1"

    # Escape special FTS5 characters in query
    safe_query = _escape_fts_query(query)

    with db.connection() as conn:
        # Query using FTS5 with BM25 ranking
        sql = f"""
            SELECT
                f.id,
                f.content,
                f.context,
                f.source,
                f.confidence,
                f.timestamp,
                bm25(facts_fts) as score
            FROM facts_fts fts
            JOIN facts f ON fts.rowid = f.id
            WHERE facts_fts MATCH ?
            AND f.confidence >= ?
            {time_clause}
            {source_clause}
            {active_clause}
            ORDER BY bm25(facts_fts)
            LIMIT ?
        """

        params = [safe_query, min_confidence] + time_params + source_params + [limit]
        rows = conn.execute(sql, params).fetchall()

        results = [
            {
                "id": row["id"],
                "content": row["content"],
                "context": row["context"],
                "source": row["source"],
                "confidence": row["confidence"],
                "timestamp": row["timestamp"],
                "score": abs(row["score"]),  # BM25 returns negative scores
            }
            for row in rows
        ]

    elapsed_ms = (time.perf_counter() - start_time) * 1000

    # Add metadata to results
    return {
        "results": results,
        "count": len(results),
        "query": query,
        "elapsed_ms": round(elapsed_ms, 2),
    }


def search_by_entity(
    entity_name: str,
    limit: int = 20,
    time_filter: TimeFilter = "all",
) -> List[Dict[str, Any]]:
    """
    Get all facts mentioning a specific entity.

    Args:
        entity_name: Entity name (e.g., '@KR8TIV', 'lucid').
        limit: Maximum results.
        time_filter: Temporal filter.

    Returns:
        List of facts linked to this entity.
    """
    db = get_db_manager()

    time_clause, time_params = _build_time_filter(time_filter)

    with db.connection() as conn:
        sql = f"""
            SELECT
                f.id,
                f.content,
                f.context,
                f.source,
                f.confidence,
                f.timestamp,
                e.name as entity_name,
                e.type as entity_type
            FROM facts f
            JOIN entity_mentions em ON f.id = em.fact_id
            JOIN entities e ON em.entity_id = e.id
            WHERE e.name = ?
            AND f.is_active = 1
            {time_clause}
            ORDER BY f.timestamp DESC
            LIMIT ?
        """

        params = [entity_name] + time_params + [limit]
        rows = conn.execute(sql, params).fetchall()

        return [
            {
                "id": row["id"],
                "content": row["content"],
                "context": row["context"],
                "source": row["source"],
                "confidence": row["confidence"],
                "timestamp": row["timestamp"],
                "entity_name": row["entity_name"],
                "entity_type": row["entity_type"],
            }
            for row in rows
        ]


def search_by_source(
    source: SourceFilter,
    limit: int = 20,
    time_filter: TimeFilter = "all",
) -> List[Dict[str, Any]]:
    """
    Get facts from a specific source system.

    Args:
        source: Source system name.
        limit: Maximum results.
        time_filter: Temporal filter.

    Returns:
        List of facts from this source.
    """
    db = get_db_manager()

    time_clause, time_params = _build_time_filter(time_filter)

    with db.connection() as conn:
        sql = f"""
            SELECT
                id,
                content,
                context,
                source,
                confidence,
                timestamp
            FROM facts
            WHERE source = ?
            AND is_active = 1
            {time_clause}
            ORDER BY timestamp DESC
            LIMIT ?
        """

        params = [source] + time_params + [limit]
        rows = conn.execute(sql, params).fetchall()

        return [
            {
                "id": row["id"],
                "content": row["content"],
                "context": row["context"],
                "source": row["source"],
                "confidence": row["confidence"],
                "timestamp": row["timestamp"],
            }
            for row in rows
        ]


def get_recent_facts(
    limit: int = 20,
    source: Optional[SourceFilter] = None,
) -> List[Dict[str, Any]]:
    """
    Get most recent facts.

    Args:
        limit: Maximum results.
        source: Optional source filter.

    Returns:
        List of recent facts.
    """
    db = get_db_manager()

    source_clause = ""
    source_params: List[Any] = []
    if source:
        source_clause = "AND source = ?"
        source_params = [source]

    with db.connection() as conn:
        sql = f"""
            SELECT
                id,
                content,
                context,
                source,
                confidence,
                timestamp
            FROM facts
            WHERE is_active = 1
            {source_clause}
            ORDER BY timestamp DESC
            LIMIT ?
        """

        params = source_params + [limit]
        rows = conn.execute(sql, params).fetchall()

        return [dict(row) for row in rows]


def get_entity_summary(entity_name: str) -> Optional[Dict[str, Any]]:
    """
    Get entity with fact count and recent activity.

    Args:
        entity_name: Entity name.

    Returns:
        Entity dict with summary stats, or None if not found.
    """
    db = get_db_manager()

    with db.connection() as conn:
        # Get entity
        entity = conn.execute(
            "SELECT * FROM entities WHERE name = ?",
            (entity_name,)
        ).fetchone()

        if not entity:
            return None

        # Get fact count
        count = conn.execute(
            """
            SELECT COUNT(*) as count FROM entity_mentions
            WHERE entity_id = ?
            """,
            (entity["id"],)
        ).fetchone()["count"]

        # Get most recent fact
        recent = conn.execute(
            """
            SELECT f.timestamp FROM facts f
            JOIN entity_mentions em ON f.id = em.fact_id
            WHERE em.entity_id = ?
            ORDER BY f.timestamp DESC
            LIMIT 1
            """,
            (entity["id"],)
        ).fetchone()

        return {
            "id": entity["id"],
            "name": entity["name"],
            "type": entity["type"],
            "summary": entity["summary"],
            "fact_count": count,
            "last_mentioned": recent["timestamp"] if recent else None,
        }


def get_facts_count(
    source: Optional[SourceFilter] = None,
    time_filter: TimeFilter = "all",
) -> int:
    """
    Get count of facts matching filters.

    Args:
        source: Optional source filter.
        time_filter: Temporal filter.

    Returns:
        Count of matching facts.
    """
    db = get_db_manager()

    time_clause, time_params = _build_time_filter(time_filter)

    source_clause = ""
    source_params: List[Any] = []
    if source:
        source_clause = "AND source = ?"
        source_params = [source]

    with db.connection() as conn:
        sql = f"""
            SELECT COUNT(*) as count FROM facts
            WHERE is_active = 1
            {time_clause}
            {source_clause}
        """

        params = time_params + source_params
        result = conn.execute(sql, params).fetchone()
        return result["count"]


def _build_time_filter(time_filter: TimeFilter) -> tuple:
    """
    Build SQL time filter clause.

    Returns:
        Tuple of (sql_clause, params).
    """
    now = datetime.utcnow()

    if time_filter == "all":
        return "", []
    elif time_filter == "today":
        cutoff = now.replace(hour=0, minute=0, second=0, microsecond=0)
    elif time_filter == "week":
        cutoff = now - timedelta(days=7)
    elif time_filter == "month":
        cutoff = now - timedelta(days=30)
    elif time_filter == "quarter":
        cutoff = now - timedelta(days=90)
    elif time_filter == "year":
        cutoff = now - timedelta(days=365)
    else:
        return "", []

    return "AND f.timestamp >= ?", [cutoff.isoformat()]


def _escape_fts_query(query: str) -> str:
    """
    Escape special FTS5 characters in query.

    FTS5 special characters: * - " ( ) : ^

    Args:
        query: Raw query string.

    Returns:
        Escaped query safe for FTS5 MATCH.
    """
    # For simple queries, just wrap tokens in quotes to match literally
    # This avoids issues with special characters

    # Split on whitespace, wrap each term in quotes
    tokens = query.split()

    # Filter out empty tokens
    tokens = [t for t in tokens if t.strip()]

    if not tokens:
        return '""'  # Empty query

    # For single word, use as-is (common case)
    if len(tokens) == 1:
        # Escape quotes within the token
        token = tokens[0].replace('"', '""')
        return f'"{token}"'

    # For multiple words, create phrase or OR query
    escaped_tokens = [t.replace('"', '""') for t in tokens]

    # Use OR to match any term (more flexible than phrase)
    return " OR ".join(f'"{t}"' for t in escaped_tokens)
```

Update core/memory/__init__.py to export search functions:

```python
"""Jarvis Memory System - Clawdbot-inspired dual-layer memory architecture."""
from .config import MemoryConfig, get_config
from .workspace import init_workspace, get_memory_path, MEMORY_ROOT
from .database import init_database, get_connection, get_db_manager, DatabaseManager
from .schema import SCHEMA_SQL, FTS5_SQL, TRIGGERS_SQL, INDEXES_SQL
from .retain import retain_fact, retain_preference, get_or_create_entity, get_user_preferences
from .markdown_sync import (
    append_to_daily_log,
    sync_fact_to_markdown,
    extract_entities_from_text,
    get_daily_log_path,
)
from .search import (
    search_facts,
    search_by_entity,
    search_by_source,
    get_recent_facts,
    get_entity_summary,
    get_facts_count,
)

__all__ = [
    # Config
    "MemoryConfig",
    "get_config",
    # Workspace
    "init_workspace",
    "get_memory_path",
    "MEMORY_ROOT",
    # Database
    "init_database",
    "get_connection",
    "get_db_manager",
    "DatabaseManager",
    # Schema
    "SCHEMA_SQL",
    "FTS5_SQL",
    "TRIGGERS_SQL",
    "INDEXES_SQL",
    # Retain
    "retain_fact",
    "retain_preference",
    "get_or_create_entity",
    "get_user_preferences",
    # Markdown
    "append_to_daily_log",
    "sync_fact_to_markdown",
    "extract_entities_from_text",
    "get_daily_log_path",
    # Search
    "search_facts",
    "search_by_entity",
    "search_by_source",
    "get_recent_facts",
    "get_entity_summary",
    "get_facts_count",
]
```
  </action>
  <verify>
```bash
python -c "
from core.memory import (
    init_workspace, init_database, retain_fact,
    search_facts, search_by_entity, get_recent_facts, get_facts_count
)

# Initialize
init_workspace()
init_database()

# Store test facts
retain_fact('KR8TIV graduated from bags.fm with strong bonding curve', context='monitoring', source='bags_intel')
retain_fact('Bought KR8TIV at \$0.05 after graduation signal', context='trade entry', source='treasury')
retain_fact('KR8TIV sold at \$0.12 for +140% profit', context='trade exit', source='treasury')
retain_fact('BONK showing bullish momentum', context='market analysis', source='telegram')

# Test FTS5 search
print('=== FTS5 Search Test ===')
results = search_facts('KR8TIV', limit=5)
print(f'Query: KR8TIV')
print(f'Found: {results[\"count\"]} results in {results[\"elapsed_ms\"]}ms')
for r in results['results']:
    print(f'  - {r[\"content\"][:50]}... (score: {r[\"score\"]:.3f})')

# Test entity search
print('\n=== Entity Search Test ===')
entity_facts = search_by_entity('KR8TIV', limit=5)
print(f'Facts mentioning KR8TIV: {len(entity_facts)}')

# Test recent facts
print('\n=== Recent Facts Test ===')
recent = get_recent_facts(limit=3)
print(f'Recent facts: {len(recent)}')
for f in recent:
    print(f'  - [{f[\"source\"]}] {f[\"content\"][:40]}...')

# Test count
print('\n=== Count Test ===')
total = get_facts_count()
treasury_count = get_facts_count(source='treasury')
print(f'Total facts: {total}')
print(f'Treasury facts: {treasury_count}')

# Verify <100ms latency
assert results['elapsed_ms'] < 100, f'Search too slow: {results[\"elapsed_ms\"]}ms'
print('\nSUCCESS - All tests passed!')
"
```
  </verify>
  <done>
- search_facts() uses FTS5 MATCH with BM25 ranking
- Search latency <100ms verified
- Temporal filters work (today, week, month, quarter, year, all)
- Source filtering works
- search_by_entity() returns facts linked to entity
- search_by_source() returns facts from source system
- get_recent_facts() returns chronologically ordered facts
- get_facts_count() returns filtered counts
  </done>
</task>

<task type="auto">
  <name>Task 2: Add search performance benchmarks</name>
  <files>core/memory/search.py</files>
  <action>
Add a benchmark function to search.py for performance validation.

Add to end of search.py:

```python
def benchmark_search(iterations: int = 100) -> Dict[str, Any]:
    """
    Benchmark search performance.

    Args:
        iterations: Number of iterations per query type.

    Returns:
        Dict with timing statistics.
    """
    import statistics

    db = get_db_manager()

    # Ensure we have some data
    fact_count = get_facts_count()
    if fact_count < 10:
        # Add sample data for benchmarking
        sample_facts = [
            "bags.fm graduation detected for token XYZ",
            "Strong bonding curve activity on KR8TIV",
            "User lucid executed trade on Jupiter",
            "Telegram command received for portfolio check",
            "X post scheduled about market conditions",
        ]
        for content in sample_facts:
            retain_fact(content, context="benchmark", source="system")

    queries = [
        "bags.fm",
        "graduation",
        "trade",
        "KR8TIV",
        "bonding curve",
    ]

    results = {"queries": {}, "summary": {}}
    all_times = []

    for query in queries:
        times = []
        for _ in range(iterations):
            start = time.perf_counter()
            search_facts(query, limit=10)
            elapsed = (time.perf_counter() - start) * 1000
            times.append(elapsed)

        results["queries"][query] = {
            "min_ms": round(min(times), 2),
            "max_ms": round(max(times), 2),
            "avg_ms": round(statistics.mean(times), 2),
            "p95_ms": round(sorted(times)[int(0.95 * len(times))], 2),
        }
        all_times.extend(times)

    results["summary"] = {
        "iterations_per_query": iterations,
        "total_queries": len(queries) * iterations,
        "overall_avg_ms": round(statistics.mean(all_times), 2),
        "overall_p95_ms": round(sorted(all_times)[int(0.95 * len(all_times))], 2),
        "meets_target": sorted(all_times)[int(0.95 * len(all_times))] < 100,
    }

    return results
```
  </action>
  <verify>
```bash
python -c "
from core.memory import init_workspace, init_database
from core.memory.search import benchmark_search

init_workspace()
init_database()

print('Running search benchmark (100 iterations per query)...')
results = benchmark_search(iterations=100)

print('\n=== Query Performance ===')
for query, stats in results['queries'].items():
    print(f'{query}: avg={stats[\"avg_ms\"]:.2f}ms, p95={stats[\"p95_ms\"]:.2f}ms')

print('\n=== Summary ===')
summary = results['summary']
print(f'Total queries: {summary[\"total_queries\"]}')
print(f'Overall avg: {summary[\"overall_avg_ms\"]:.2f}ms')
print(f'Overall p95: {summary[\"overall_p95_ms\"]:.2f}ms')
print(f'Meets <100ms target: {summary[\"meets_target\"]}')

assert summary['meets_target'], 'Search performance does not meet target!'
print('\nBENCHMARK PASSED!')
"
```
  </verify>
  <done>
- benchmark_search() runs multiple query types
- Reports min, max, avg, p95 latency per query
- Verifies <100ms p95 target is met
  </done>
</task>

</tasks>

<verification>
1. Test search with various queries:
```bash
python -c "
from core.memory import search_facts

# Different query types
queries = ['bags.fm', 'KR8TIV graduation', 'trade profit']
for q in queries:
    r = search_facts(q)
    print(f'{q}: {r[\"count\"]} results in {r[\"elapsed_ms\"]}ms')
"
```

2. Test temporal filters:
```bash
python -c "
from core.memory import search_facts

for tf in ['today', 'week', 'month', 'all']:
    r = search_facts('trade', time_filter=tf)
    print(f'{tf}: {r[\"count\"]} results')
"
```

3. Run full benchmark:
```bash
python -c "
from core.memory.search import benchmark_search
from core.memory import init_workspace, init_database
init_workspace()
init_database()
results = benchmark_search(50)
print(f'p95 latency: {results[\"summary\"][\"overall_p95_ms\"]}ms')
"
```
</verification>

<success_criteria>
- search_facts() returns results with BM25 scores
- FTS5 MATCH syntax works correctly
- Temporal filters restrict results appropriately
- Source filters restrict results appropriately
- Query latency <100ms at p95 (verified by benchmark)
- search_by_entity() returns entity-linked facts
- get_recent_facts() returns chronologically ordered results
- Special characters in queries don't cause errors
</success_criteria>

<output>
After completion, create `.planning/memory-integration/phases/06-memory-foundation/06-04-SUMMARY.md`
</output>
