---
phase: 06-memory-foundation
plan: 06
type: execute
wave: 4
depends_on: ["06-03", "06-05"]
files_modified:
  - core/memory/migration.py
  - scripts/migrate_archival_memory.py
autonomous: true

must_haves:
  truths:
    - "Existing 100+ PostgreSQL archival_memory learnings migrated to SQLite facts table"
    - "fact_embeddings table links migrated facts to original PostgreSQL IDs"
    - "Migration is idempotent (can run multiple times safely)"
    - "State stored in ~/.lifeos/memory/ per INT-006 requirement"
    - "Migration script reports count of migrated entries"
  artifacts:
    - path: "core/memory/migration.py"
      provides: "Migration utilities for PostgreSQL to SQLite"
      exports: ["migrate_archival_memory", "get_migration_status"]
    - path: "scripts/migrate_archival_memory.py"
      provides: "Standalone migration script"
  key_links:
    - from: "core/memory/migration.py"
      to: "core/memory/postgres_integration.py"
      via: "list_archival_memories()"
      pattern: "list_archival_memories"
    - from: "core/memory/migration.py"
      to: "core/memory/retain.py"
      via: "retain_fact()"
      pattern: "retain_fact"
---

<objective>
Migrate existing 100+ PostgreSQL archival_memory learnings to the new SQLite facts table, establishing bidirectional links via fact_embeddings table.

Purpose: Preserve all existing semantic memory learnings while transitioning to the new dual-layer architecture. This ensures no data loss and maintains the ability to leverage existing BGE embeddings for hybrid search.

Output:
- core/memory/migration.py with migration utilities
- scripts/migrate_archival_memory.py standalone migration script
- 100+ facts migrated from PostgreSQL to SQLite
- fact_embeddings table populated with postgres_memory_id links
</objective>

<execution_context>
@C:\Users\lucid\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\lucid\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/memory-integration/PROJECT.md
@.planning/memory-integration/phases/06-memory-foundation/06-RESEARCH.md
@.planning/memory-integration/phases/06-memory-foundation/06-05-SUMMARY.md

From PROJECT.md:
- Existing PostgreSQL archival_memory has 100+ learnings with BGE embeddings
- DATABASE_URL environment variable contains connection string
- No data loss requirement - migrate all existing learnings

From REQUIREMENTS.md:
- INT-006: Store state in `~/.lifeos/memory/` alongside existing trading state

PostgreSQL archival_memory schema:
```sql
CREATE TABLE archival_memory (
    id SERIAL PRIMARY KEY,
    content TEXT NOT NULL,
    metadata JSONB,
    embedding VECTOR(1024),  -- BGE embeddings
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create migration module</name>
  <files>core/memory/migration.py</files>
  <action>
Create migration.py with utilities for PostgreSQL to SQLite migration.

```python
"""Migration utilities for PostgreSQL archival_memory to SQLite facts."""
import json
from datetime import datetime
from typing import Optional, Dict, Any, List, Tuple

from .postgres_integration import (
    is_postgres_available,
    get_postgres_connection,
    get_archival_memory_count,
    list_archival_memories,
)
from .database import get_db_manager
from .retain import retain_fact


def get_migration_status() -> Dict[str, Any]:
    """
    Get current migration status.

    Returns:
        Dict with postgres_count, sqlite_count, migrated_count, pending_count.
    """
    # Get PostgreSQL count
    postgres_count = get_archival_memory_count() if is_postgres_available() else 0

    # Get SQLite counts
    db = get_db_manager()
    with db.connection() as conn:
        # Total facts in SQLite
        sqlite_count = conn.execute(
            "SELECT COUNT(*) as cnt FROM facts"
        ).fetchone()["cnt"]

        # Facts with PostgreSQL links (already migrated)
        migrated_count = conn.execute(
            "SELECT COUNT(*) as cnt FROM fact_embeddings WHERE postgres_memory_id IS NOT NULL"
        ).fetchone()["cnt"]

    return {
        "postgres_available": is_postgres_available(),
        "postgres_count": postgres_count,
        "sqlite_count": sqlite_count,
        "migrated_count": migrated_count,
        "pending_count": max(0, postgres_count - migrated_count),
    }


def get_migrated_postgres_ids() -> set:
    """
    Get set of PostgreSQL IDs that have already been migrated.

    Returns:
        Set of postgres_memory_id values.
    """
    db = get_db_manager()
    with db.connection() as conn:
        rows = conn.execute(
            "SELECT postgres_memory_id FROM fact_embeddings WHERE postgres_memory_id IS NOT NULL"
        ).fetchall()
        return {row["postgres_memory_id"] for row in rows}


def migrate_single_entry(
    postgres_id: int,
    content: str,
    metadata: Optional[Dict],
    created_at: Optional[datetime] = None,
) -> int:
    """
    Migrate a single archival_memory entry to SQLite.

    Args:
        postgres_id: PostgreSQL archival_memory.id
        content: Memory content
        metadata: JSONB metadata (may contain context, tags, etc.)
        created_at: Original creation timestamp

    Returns:
        SQLite fact_id
    """
    # Extract context from metadata if present
    context = None
    source = "archival_memory"
    entities = []

    if metadata:
        if isinstance(metadata, str):
            try:
                metadata = json.loads(metadata)
            except json.JSONDecodeError:
                metadata = {}

        context = metadata.get("context")
        if not context:
            # Try other common metadata fields
            context = metadata.get("session_id") or metadata.get("type")

        # Extract tags as potential entities
        tags = metadata.get("tags", [])
        if isinstance(tags, str):
            tags = [t.strip() for t in tags.split(",")]
        entities = [f"#{tag}" for tag in tags if tag]

    # Store via retain_fact (handles SQLite insert + Markdown sync)
    fact_id = retain_fact(
        content=content,
        context=context or "migrated from archival_memory",
        entities=entities if entities else None,
        source=source,
        confidence=1.0,
        auto_extract_entities=True,  # Also extract from content
    )

    # Link to PostgreSQL ID
    db = get_db_manager()
    with db.transaction() as conn:
        conn.execute(
            """
            INSERT OR REPLACE INTO fact_embeddings (fact_id, postgres_memory_id)
            VALUES (?, ?)
            """,
            (fact_id, postgres_id)
        )

    return fact_id


def migrate_archival_memory(
    batch_size: int = 50,
    skip_existing: bool = True,
    verbose: bool = True,
) -> Dict[str, Any]:
    """
    Migrate all archival_memory entries from PostgreSQL to SQLite.

    Args:
        batch_size: Number of entries to fetch per batch
        skip_existing: Skip entries already migrated (idempotent)
        verbose: Print progress

    Returns:
        Dict with migrated_count, skipped_count, error_count, errors
    """
    if not is_postgres_available():
        return {
            "success": False,
            "error": "PostgreSQL not available (DATABASE_URL not set)",
            "migrated_count": 0,
            "skipped_count": 0,
            "error_count": 0,
        }

    # Get already migrated IDs
    existing_ids = get_migrated_postgres_ids() if skip_existing else set()

    if verbose:
        print(f"Found {len(existing_ids)} already migrated entries")

    # Track progress
    migrated_count = 0
    skipped_count = 0
    error_count = 0
    errors: List[Tuple[int, str]] = []

    # Fetch all entries from PostgreSQL
    offset = 0
    while True:
        try:
            with get_postgres_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT id, content, metadata, created_at
                    FROM archival_memory
                    ORDER BY id
                    LIMIT %s OFFSET %s
                """, (batch_size, offset))

                rows = cursor.fetchall()

                if not rows:
                    break  # No more entries

                for row in rows:
                    postgres_id = row["id"]

                    # Skip if already migrated
                    if postgres_id in existing_ids:
                        skipped_count += 1
                        continue

                    try:
                        fact_id = migrate_single_entry(
                            postgres_id=postgres_id,
                            content=row["content"],
                            metadata=row["metadata"],
                            created_at=row.get("created_at"),
                        )
                        migrated_count += 1

                        if verbose and migrated_count % 10 == 0:
                            print(f"Migrated {migrated_count} entries...")

                    except Exception as e:
                        error_count += 1
                        errors.append((postgres_id, str(e)))
                        if verbose:
                            print(f"Error migrating {postgres_id}: {e}")

                offset += batch_size

        except Exception as e:
            return {
                "success": False,
                "error": f"PostgreSQL query failed: {e}",
                "migrated_count": migrated_count,
                "skipped_count": skipped_count,
                "error_count": error_count,
                "errors": errors,
            }

    if verbose:
        print(f"\nMigration complete!")
        print(f"  Migrated: {migrated_count}")
        print(f"  Skipped (existing): {skipped_count}")
        print(f"  Errors: {error_count}")

    return {
        "success": True,
        "migrated_count": migrated_count,
        "skipped_count": skipped_count,
        "error_count": error_count,
        "errors": errors[:10] if errors else [],  # First 10 errors
    }


def verify_migration() -> Dict[str, Any]:
    """
    Verify migration completeness.

    Returns:
        Dict with verification results.
    """
    status = get_migration_status()

    # Check if all entries are migrated
    is_complete = status["pending_count"] == 0

    # Sample verification: check a few random entries
    db = get_db_manager()
    sample_facts = []
    with db.connection() as conn:
        rows = conn.execute("""
            SELECT f.id, f.content, fe.postgres_memory_id
            FROM facts f
            JOIN fact_embeddings fe ON f.id = fe.fact_id
            WHERE fe.postgres_memory_id IS NOT NULL
            LIMIT 5
        """).fetchall()
        sample_facts = [dict(row) for row in rows]

    return {
        "is_complete": is_complete,
        "status": status,
        "sample_facts": sample_facts,
    }
```
  </action>
  <verify>
```bash
python -c "
from core.memory.migration import get_migration_status

status = get_migration_status()
print('=== Migration Status ===')
print(f'PostgreSQL available: {status[\"postgres_available\"]}')
print(f'PostgreSQL entries: {status[\"postgres_count\"]}')
print(f'SQLite facts: {status[\"sqlite_count\"]}')
print(f'Already migrated: {status[\"migrated_count\"]}')
print(f'Pending: {status[\"pending_count\"]}')
print('SUCCESS')
"
```
  </verify>
  <done>
- get_migration_status() reports PostgreSQL and SQLite counts
- migrate_single_entry() migrates one entry with fact_embeddings link
- migrate_archival_memory() batch migrates all entries (idempotent)
- verify_migration() confirms completeness
  </done>
</task>

<task type="auto">
  <name>Task 2: Create standalone migration script</name>
  <files>scripts/migrate_archival_memory.py</files>
  <action>
Create standalone migration script that can be run from command line.

```python
#!/usr/bin/env python3
"""
Migrate PostgreSQL archival_memory to SQLite facts.

Usage:
    python scripts/migrate_archival_memory.py
    python scripts/migrate_archival_memory.py --status
    python scripts/migrate_archival_memory.py --verify
    python scripts/migrate_archival_memory.py --dry-run
"""
import argparse
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.memory import init_workspace, init_database
from core.memory.migration import (
    get_migration_status,
    migrate_archival_memory,
    verify_migration,
)


def main():
    parser = argparse.ArgumentParser(
        description="Migrate PostgreSQL archival_memory to SQLite facts"
    )
    parser.add_argument(
        "--status",
        action="store_true",
        help="Show migration status without migrating"
    )
    parser.add_argument(
        "--verify",
        action="store_true",
        help="Verify migration completeness"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be migrated without actually migrating"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=50,
        help="Batch size for migration (default: 50)"
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="Re-migrate entries even if already migrated"
    )

    args = parser.parse_args()

    # Initialize workspace and database
    print("Initializing memory workspace...")
    init_workspace()
    init_database()

    if args.status or args.dry_run:
        status = get_migration_status()
        print("\n=== Migration Status ===")
        print(f"PostgreSQL available: {status['postgres_available']}")
        print(f"PostgreSQL entries:   {status['postgres_count']}")
        print(f"SQLite facts:         {status['sqlite_count']}")
        print(f"Already migrated:     {status['migrated_count']}")
        print(f"Pending migration:    {status['pending_count']}")

        if args.dry_run:
            print(f"\n[DRY RUN] Would migrate {status['pending_count']} entries")
        return 0

    if args.verify:
        result = verify_migration()
        print("\n=== Migration Verification ===")
        print(f"Complete: {result['is_complete']}")
        print(f"Pending: {result['status']['pending_count']}")

        if result['sample_facts']:
            print("\nSample migrated facts:")
            for fact in result['sample_facts']:
                content = fact['content'][:60] + "..." if len(fact['content']) > 60 else fact['content']
                print(f"  [{fact['id']}] pg:{fact['postgres_memory_id']} - {content}")

        return 0 if result['is_complete'] else 1

    # Run migration
    print("\n=== Starting Migration ===")
    result = migrate_archival_memory(
        batch_size=args.batch_size,
        skip_existing=not args.force,
        verbose=True,
    )

    if not result['success']:
        print(f"\nMigration failed: {result.get('error', 'Unknown error')}")
        return 1

    print("\n=== Migration Summary ===")
    print(f"Migrated: {result['migrated_count']}")
    print(f"Skipped:  {result['skipped_count']}")
    print(f"Errors:   {result['error_count']}")

    if result['errors']:
        print("\nFirst few errors:")
        for pg_id, error in result['errors'][:5]:
            print(f"  [{pg_id}] {error}")

    # Verify after migration
    verification = verify_migration()
    if verification['is_complete']:
        print("\nMigration verified complete!")
    else:
        print(f"\nWarning: {verification['status']['pending_count']} entries still pending")

    return 0


if __name__ == "__main__":
    sys.exit(main())
```
  </action>
  <verify>
```bash
python scripts/migrate_archival_memory.py --status
```
Should show migration status without errors.
  </verify>
  <done>
- Script runs from command line
- --status shows current migration state
- --verify confirms completeness
- --dry-run shows what would be migrated
- --force re-migrates existing entries
- Proper exit codes for scripting
  </done>
</task>

<task type="auto">
  <name>Task 3: Run migration and verify INT-006 compliance</name>
  <files>core/memory/__init__.py</files>
  <action>
1. Update core/memory/__init__.py to export migration functions
2. Run the migration
3. Verify data is stored in ~/.lifeos/memory/ per INT-006

Update exports:
```python
from .migration import (
    get_migration_status,
    migrate_archival_memory,
    verify_migration,
)

# Add to __all__
__all__ = [
    # ... existing exports ...
    # Migration
    "get_migration_status",
    "migrate_archival_memory",
    "verify_migration",
]
```

Verification steps:
1. Run migration script
2. Confirm facts exist in ~/.lifeos/memory/jarvis.db
3. Confirm daily logs created in ~/.lifeos/memory/memory/
4. Confirm fact_embeddings links populated
  </action>
  <verify>
```bash
# Run migration
python scripts/migrate_archival_memory.py

# Verify INT-006 compliance (state in ~/.lifeos/memory/)
python -c "
from pathlib import Path
import os

# Check ~/.lifeos/memory/ exists
memory_root = Path.home() / '.lifeos' / 'memory'
assert memory_root.exists(), f'Missing: {memory_root}'

# Check jarvis.db exists
db_path = memory_root / 'jarvis.db'
assert db_path.exists(), f'Missing: {db_path}'

# Check some facts were migrated
import sqlite3
conn = sqlite3.connect(str(db_path))
conn.row_factory = sqlite3.Row
count = conn.execute('SELECT COUNT(*) as cnt FROM facts').fetchone()['cnt']
linked = conn.execute('SELECT COUNT(*) as cnt FROM fact_embeddings WHERE postgres_memory_id IS NOT NULL').fetchone()['cnt']
conn.close()

print(f'INT-006 Compliance Check:')
print(f'  State directory: {memory_root} [EXISTS]')
print(f'  Database: {db_path} [EXISTS]')
print(f'  Facts stored: {count}')
print(f'  PostgreSQL links: {linked}')
print('INT-006 VERIFIED: State stored in ~/.lifeos/memory/')
"
```
  </verify>
  <done>
- Migration exports added to core/memory/__init__.py
- Migration run successfully
- 100+ entries migrated from PostgreSQL to SQLite
- fact_embeddings table links established
- State stored in ~/.lifeos/memory/ per INT-006
  </done>
</task>

</tasks>

<verification>
1. Check migration status:
```bash
python scripts/migrate_archival_memory.py --status
```

2. Run migration:
```bash
python scripts/migrate_archival_memory.py
```

3. Verify completion:
```bash
python scripts/migrate_archival_memory.py --verify
```

4. Verify INT-006 path compliance:
```bash
ls -la ~/.lifeos/memory/
ls -la ~/.lifeos/memory/jarvis.db
```

5. Check fact_embeddings links:
```bash
python -c "
from core.memory import get_db_manager, init_workspace, init_database
init_workspace()
init_database()
db = get_db_manager()
with db.connection() as conn:
    count = conn.execute('SELECT COUNT(*) FROM fact_embeddings WHERE postgres_memory_id IS NOT NULL').fetchone()[0]
    print(f'PostgreSQL links in fact_embeddings: {count}')
"
```
</verification>

<success_criteria>
- migrate_archival_memory() migrates all PostgreSQL entries to SQLite
- Migration is idempotent (running twice doesn't duplicate data)
- fact_embeddings table links each migrated fact to its PostgreSQL ID
- 100+ entries confirmed migrated (matches PostgreSQL count)
- State stored in ~/.lifeos/memory/ per INT-006
- Daily logs created in ~/.lifeos/memory/memory/
- Migration script runnable from command line
- verify_migration() confirms completeness
</success_criteria>

<output>
After completion, create `.planning/memory-integration/phases/06-memory-foundation/06-06-SUMMARY.md`
</output>
