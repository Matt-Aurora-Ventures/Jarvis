---
phase: 06-memory-foundation
plan: 05
type: execute
wave: 3
depends_on: ["06-03", "06-04"]
files_modified:
  - core/memory/postgres_integration.py
  - core/memory/hybrid_search.py
  - core/memory/recall.py
autonomous: true

must_haves:
  truths:
    - "PostgreSQL archival_memory table is accessible from memory system"
    - "fact_embeddings table links SQLite facts to PostgreSQL IDs"
    - "Hybrid search combines FTS5 (keyword) + vector (semantic) results"
    - "RRF fusion produces ranked results from both sources"
    - "recall() function provides unified search interface"
  artifacts:
    - path: "core/memory/postgres_integration.py"
      provides: "PostgreSQL connection and vector search"
      exports: ["get_postgres_connection", "vector_search", "store_embedding"]
    - path: "core/memory/hybrid_search.py"
      provides: "RRF fusion of FTS5 and vector search"
      exports: ["hybrid_search", "rrf_fusion"]
    - path: "core/memory/recall.py"
      provides: "Unified recall interface"
      exports: ["recall"]
  key_links:
    - from: "core/memory/hybrid_search.py"
      to: "core/memory/search.py"
      via: "search_facts()"
      pattern: "search_facts"
    - from: "core/memory/hybrid_search.py"
      to: "core/memory/postgres_integration.py"
      via: "vector_search()"
      pattern: "vector_search"
---

<!--
  Note: This plan intentionally combines PostgreSQL integration, hybrid search, and recall
  into a single plan despite the complexity because:
  1. All three components are tightly coupled (recall uses hybrid_search uses postgres_integration)
  2. Splitting would create unnecessary Wave dependencies
  3. The code shares significant context (RRF algorithm, embedding handling)
  4. Testing requires all three components together

  The migration of existing data is handled separately in Plan 06.
-->

<objective>
Integrate existing PostgreSQL archival_memory (100+ learnings with BGE embeddings) with the new SQLite memory system using Reciprocal Rank Fusion (RRF) for hybrid search.

Purpose: Combine fast keyword search (FTS5) with semantic search (PostgreSQL vectors) to enable intelligent memory retrieval. This preserves the existing 100+ learnings while adding the new dual-layer architecture.

Output:
- core/memory/postgres_integration.py for PostgreSQL access
- core/memory/hybrid_search.py with RRF fusion
- core/memory/recall.py with unified recall() function
</objective>

<execution_context>
@C:\Users\lucid\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\lucid\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/memory-integration/PROJECT.md
@.planning/memory-integration/phases/06-memory-foundation/06-RESEARCH.md

From RESEARCH.md Pattern 2: Hybrid Search (FTS5 + Vector):
```python
def recall(query: str, k: int = 5, filters: dict = None) -> list[dict]:
    """Hybrid search with RRF fusion"""
    # 1. FTS5 keyword search (SQLite)
    # 2. Vector semantic search (PostgreSQL)
    # 3. Reciprocal Rank Fusion (RRF)
    # Formula: RRF_score = sum(1 / (rank + k)) where k=60
    # 4. Return top k facts with context
```

From PROJECT.md:
- Existing PostgreSQL archival_memory has 100+ learnings with BGE embeddings (bge-large-en-v1.5, 1024-dim)
- DATABASE_URL environment variable contains connection string

Requirements:
- INT-001: Migrate existing PostgreSQL archival_memory learnings to new schema
- INT-002: Link SQLite fact_embeddings to PostgreSQL archival_memory.id
- INT-003: Preserve existing semantic search functionality
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create PostgreSQL integration module</name>
  <files>core/memory/postgres_integration.py</files>
  <action>
Create postgres_integration.py with PostgreSQL connection and vector search.

```python
"""PostgreSQL integration for vector embeddings and archival memory."""
import os
from typing import Optional, List, Dict, Any, Tuple
from contextlib import contextmanager

try:
    import psycopg2
    import psycopg2.extras
    PSYCOPG2_AVAILABLE = True
except ImportError:
    PSYCOPG2_AVAILABLE = False
    psycopg2 = None

from .config import get_config


# Connection pool (simple thread-local for now)
_connection: Optional[Any] = None


def is_postgres_available() -> bool:
    """Check if PostgreSQL is configured and available."""
    if not PSYCOPG2_AVAILABLE:
        return False

    config = get_config()
    return config.postgres_url is not None


@contextmanager
def get_postgres_connection():
    """
    Get PostgreSQL connection.

    Uses connection string from DATABASE_URL environment variable.

    Yields:
        psycopg2 connection object.

    Raises:
        RuntimeError: If PostgreSQL is not configured.
    """
    global _connection

    if not PSYCOPG2_AVAILABLE:
        raise RuntimeError("psycopg2 not installed. Run: pip install psycopg2-binary")

    config = get_config()
    if not config.postgres_url:
        raise RuntimeError("DATABASE_URL environment variable not set")

    if _connection is None or _connection.closed:
        _connection = psycopg2.connect(
            config.postgres_url,
            cursor_factory=psycopg2.extras.RealDictCursor
        )

    try:
        yield _connection
    except Exception:
        _connection.rollback()
        raise


def check_archival_memory_exists() -> bool:
    """
    Check if archival_memory table exists in PostgreSQL.

    Returns:
        True if table exists.
    """
    if not is_postgres_available():
        return False

    try:
        with get_postgres_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables
                    WHERE table_name = 'archival_memory'
                )
            """)
            result = cursor.fetchone()
            return result['exists'] if result else False
    except Exception:
        return False


def get_archival_memory_count() -> int:
    """
    Get count of entries in archival_memory table.

    Returns:
        Number of entries, or 0 if not available.
    """
    if not is_postgres_available():
        return 0

    try:
        with get_postgres_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) as count FROM archival_memory")
            result = cursor.fetchone()
            return result['count'] if result else 0
    except Exception:
        return 0


def vector_search(
    query_embedding: List[float],
    limit: int = 20,
    min_similarity: float = 0.3,
) -> List[Dict[str, Any]]:
    """
    Search archival_memory using vector similarity.

    Uses cosine similarity via pgvector extension.

    Args:
        query_embedding: Query vector (1024-dim for BGE).
        limit: Maximum results.
        min_similarity: Minimum cosine similarity threshold.

    Returns:
        List of results with id, content, metadata, similarity.
    """
    if not is_postgres_available():
        return []

    try:
        with get_postgres_connection() as conn:
            cursor = conn.cursor()

            # Convert to pgvector format
            embedding_str = "[" + ",".join(str(x) for x in query_embedding) + "]"

            cursor.execute("""
                SELECT
                    id,
                    content,
                    metadata,
                    1 - (embedding <=> %s::vector) as similarity
                FROM archival_memory
                WHERE 1 - (embedding <=> %s::vector) >= %s
                ORDER BY embedding <=> %s::vector
                LIMIT %s
            """, (embedding_str, embedding_str, min_similarity, embedding_str, limit))

            results = cursor.fetchall()

            return [
                {
                    "id": row["id"],
                    "content": row["content"],
                    "metadata": row["metadata"],
                    "similarity": float(row["similarity"]),
                }
                for row in results
            ]
    except Exception as e:
        # Log error but don't crash - fallback to FTS5 only
        print(f"Warning: PostgreSQL vector search failed: {e}")
        return []


def store_embedding(
    fact_id: int,
    content: str,
    embedding: List[float],
    metadata: Optional[Dict] = None,
) -> Optional[int]:
    """
    Store embedding in PostgreSQL and link to SQLite fact.

    Args:
        fact_id: SQLite fact ID.
        content: Fact content.
        embedding: Vector embedding (1024-dim).
        metadata: Optional metadata dict.

    Returns:
        PostgreSQL archival_memory ID, or None on failure.
    """
    if not is_postgres_available():
        return None

    try:
        with get_postgres_connection() as conn:
            cursor = conn.cursor()

            # Convert to pgvector format
            embedding_str = "[" + ",".join(str(x) for x in embedding) + "]"

            # Prepare metadata
            import json
            meta_json = json.dumps(metadata or {"fact_id": fact_id})

            cursor.execute("""
                INSERT INTO archival_memory (content, metadata, embedding)
                VALUES (%s, %s, %s::vector)
                RETURNING id
            """, (content, meta_json, embedding_str))

            postgres_id = cursor.fetchone()["id"]
            conn.commit()

            # Link in SQLite
            from .database import get_db_manager
            db = get_db_manager()
            with db.transaction() as sqlite_conn:
                sqlite_conn.execute("""
                    INSERT OR REPLACE INTO fact_embeddings (fact_id, postgres_memory_id)
                    VALUES (?, ?)
                """, (fact_id, postgres_id))

            return postgres_id

    except Exception as e:
        print(f"Warning: Failed to store embedding: {e}")
        return None


def get_embedding_for_fact(fact_id: int) -> Optional[int]:
    """
    Get PostgreSQL archival_memory ID linked to a SQLite fact.

    Args:
        fact_id: SQLite fact ID.

    Returns:
        PostgreSQL ID, or None if not linked.
    """
    from .database import get_db_manager

    db = get_db_manager()
    with db.connection() as conn:
        result = conn.execute(
            "SELECT postgres_memory_id FROM fact_embeddings WHERE fact_id = ?",
            (fact_id,)
        ).fetchone()

        return result["postgres_memory_id"] if result else None


def generate_embedding(text: str) -> Optional[List[float]]:
    """
    Generate BGE embedding for text.

    Uses sentence-transformers with BAAI/bge-large-en-v1.5 model.

    Args:
        text: Text to embed.

    Returns:
        1024-dim embedding vector, or None on failure.
    """
    try:
        from sentence_transformers import SentenceTransformer

        # Use cached model
        model = _get_embedding_model()
        if model is None:
            return None

        # Generate embedding
        embedding = model.encode(text, normalize_embeddings=True)
        return embedding.tolist()

    except ImportError:
        print("Warning: sentence-transformers not installed")
        return None
    except Exception as e:
        print(f"Warning: Embedding generation failed: {e}")
        return None


_embedding_model = None


def _get_embedding_model():
    """Get or load the embedding model (cached)."""
    global _embedding_model

    if _embedding_model is None:
        try:
            from sentence_transformers import SentenceTransformer
            config = get_config()
            _embedding_model = SentenceTransformer(config.embedding_model)
        except Exception as e:
            print(f"Warning: Could not load embedding model: {e}")
            return None

    return _embedding_model


def list_archival_memories(limit: int = 50) -> List[Dict[str, Any]]:
    """
    List entries from archival_memory table.

    Args:
        limit: Maximum entries to return.

    Returns:
        List of archival memory entries.
    """
    if not is_postgres_available():
        return []

    try:
        with get_postgres_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT id, content, metadata, created_at
                FROM archival_memory
                ORDER BY created_at DESC
                LIMIT %s
            """, (limit,))

            return [dict(row) for row in cursor.fetchall()]
    except Exception as e:
        print(f"Warning: Could not list archival memories: {e}")
        return []
```
  </action>
  <verify>
```bash
python -c "
from core.memory.postgres_integration import (
    is_postgres_available,
    check_archival_memory_exists,
    get_archival_memory_count,
    list_archival_memories
)

print('=== PostgreSQL Integration Check ===')
print(f'PostgreSQL available: {is_postgres_available()}')

if is_postgres_available():
    print(f'archival_memory exists: {check_archival_memory_exists()}')
    print(f'archival_memory count: {get_archival_memory_count()}')

    memories = list_archival_memories(limit=3)
    print(f'Sample memories: {len(memories)}')
    for m in memories:
        content = m.get('content', '')[:50]
        print(f'  - {content}...')
else:
    print('PostgreSQL not configured (DATABASE_URL not set)')
    print('Hybrid search will use FTS5 only (graceful degradation)')

print('SUCCESS')
"
```
  </verify>
  <done>
- PostgreSQL connection working via DATABASE_URL
- archival_memory table accessible
- vector_search() returns results with cosine similarity
- store_embedding() links facts to PostgreSQL IDs
- Graceful degradation when PostgreSQL unavailable
  </done>
</task>

<task type="auto">
  <name>Task 2: Create hybrid search with RRF fusion</name>
  <files>core/memory/hybrid_search.py</files>
  <action>
Create hybrid_search.py with Reciprocal Rank Fusion.

```python
"""Hybrid search combining FTS5 keyword search with vector semantic search."""
import time
from typing import Optional, List, Dict, Any

from .search import search_facts
from .postgres_integration import (
    is_postgres_available,
    vector_search,
    generate_embedding,
)


def rrf_fusion(
    fts_results: List[Dict[str, Any]],
    vector_results: List[Dict[str, Any]],
    k: int = 60,
) -> List[Dict[str, Any]]:
    """
    Merge results using Reciprocal Rank Fusion (RRF).

    RRF score = sum(1 / (rank + k)) for each ranking the document appears in.

    Args:
        fts_results: Results from FTS5 search (have 'id' and 'score').
        vector_results: Results from vector search (have 'id' and 'similarity').
        k: RRF constant (default 60, standard value).

    Returns:
        Merged and re-ranked results with RRF scores.
    """
    merged: Dict[int, Dict[str, Any]] = {}

    # Process FTS5 results
    for rank, result in enumerate(fts_results):
        fact_id = result.get("id")
        if fact_id is None:
            continue

        rrf_score = 1.0 / (rank + k)

        if fact_id not in merged:
            merged[fact_id] = {
                "id": fact_id,
                "content": result.get("content"),
                "context": result.get("context"),
                "source": result.get("source"),
                "confidence": result.get("confidence"),
                "timestamp": result.get("timestamp"),
                "rrf_score": rrf_score,
                "fts_rank": rank + 1,
                "fts_score": result.get("score"),
                "vector_rank": None,
                "similarity": None,
            }
        else:
            merged[fact_id]["rrf_score"] += rrf_score
            merged[fact_id]["fts_rank"] = rank + 1
            merged[fact_id]["fts_score"] = result.get("score")

    # Process vector results
    for rank, result in enumerate(vector_results):
        # Vector results use PostgreSQL ID, need to map to fact_id
        postgres_id = result.get("id")
        similarity = result.get("similarity", 0)

        # Try to find matching fact by content (fallback)
        # In production, we'd use fact_embeddings table for mapping
        fact_id = _find_fact_by_postgres_id(postgres_id, result.get("content"))

        if fact_id is None:
            # Create synthetic entry for PostgreSQL-only results
            fact_id = f"pg_{postgres_id}"

        rrf_score = 1.0 / (rank + k)

        if fact_id not in merged:
            merged[fact_id] = {
                "id": fact_id,
                "content": result.get("content"),
                "context": result.get("metadata", {}).get("context") if isinstance(result.get("metadata"), dict) else None,
                "source": "archival_memory",
                "confidence": 1.0,
                "timestamp": None,
                "rrf_score": rrf_score,
                "fts_rank": None,
                "fts_score": None,
                "vector_rank": rank + 1,
                "similarity": similarity,
            }
        else:
            merged[fact_id]["rrf_score"] += rrf_score
            merged[fact_id]["vector_rank"] = rank + 1
            merged[fact_id]["similarity"] = similarity

    # Sort by RRF score (descending)
    sorted_results = sorted(
        merged.values(),
        key=lambda x: x["rrf_score"],
        reverse=True
    )

    return sorted_results


def _find_fact_by_postgres_id(postgres_id: int, content: Optional[str]) -> Optional[int]:
    """
    Find SQLite fact_id from PostgreSQL archival_memory ID.

    Uses fact_embeddings table for mapping.
    """
    from .database import get_db_manager

    try:
        db = get_db_manager()
        with db.connection() as conn:
            # First try direct mapping
            result = conn.execute(
                "SELECT fact_id FROM fact_embeddings WHERE postgres_memory_id = ?",
                (postgres_id,)
            ).fetchone()

            if result:
                return result["fact_id"]

            # Fallback: search by content similarity (expensive, avoid in production)
            if content:
                result = conn.execute(
                    "SELECT id FROM facts WHERE content = ? LIMIT 1",
                    (content,)
                ).fetchone()

                if result:
                    return result["id"]

    except Exception:
        pass

    return None


def hybrid_search(
    query: str,
    limit: int = 10,
    time_filter: str = "all",
    source: Optional[str] = None,
    use_vector: bool = True,
    min_similarity: float = 0.3,
) -> Dict[str, Any]:
    """
    Perform hybrid search combining FTS5 and vector search.

    Uses RRF to merge and rank results from both sources.

    Args:
        query: Search query.
        limit: Maximum results to return.
        time_filter: Temporal filter for FTS5.
        source: Source filter for FTS5.
        use_vector: Whether to include vector search (requires PostgreSQL).
        min_similarity: Minimum vector similarity threshold.

    Returns:
        Dict with results, metadata, and timing.

    Example:
        results = hybrid_search("bags.fm graduation success rate", limit=5)
    """
    start_time = time.perf_counter()

    # 1. FTS5 keyword search
    fts_response = search_facts(
        query=query,
        limit=limit * 2,  # Get more for fusion
        time_filter=time_filter,
        source=source,
    )
    fts_results = fts_response.get("results", [])
    fts_time = fts_response.get("elapsed_ms", 0)

    # 2. Vector semantic search (if available)
    vector_results = []
    vector_time = 0

    if use_vector and is_postgres_available():
        vector_start = time.perf_counter()

        # Generate query embedding
        query_embedding = generate_embedding(query)

        if query_embedding:
            vector_results = vector_search(
                query_embedding=query_embedding,
                limit=limit * 2,
                min_similarity=min_similarity,
            )

        vector_time = (time.perf_counter() - vector_start) * 1000

    # 3. RRF fusion
    if vector_results:
        merged = rrf_fusion(fts_results, vector_results)
    else:
        # FTS5 only - add synthetic RRF scores
        merged = []
        for rank, result in enumerate(fts_results):
            merged.append({
                **result,
                "rrf_score": 1.0 / (rank + 60),
                "fts_rank": rank + 1,
                "vector_rank": None,
                "similarity": None,
            })

    # Limit final results
    final_results = merged[:limit]

    total_time = (time.perf_counter() - start_time) * 1000

    return {
        "results": final_results,
        "count": len(final_results),
        "query": query,
        "timing": {
            "fts_ms": round(fts_time, 2),
            "vector_ms": round(vector_time, 2),
            "total_ms": round(total_time, 2),
        },
        "sources": {
            "fts_count": len(fts_results),
            "vector_count": len(vector_results),
            "vector_available": is_postgres_available(),
        },
    }
```
  </action>
  <verify>
```bash
python -c "
from core.memory import init_workspace, init_database, retain_fact
from core.memory.hybrid_search import hybrid_search, rrf_fusion

# Initialize
init_workspace()
init_database()

# Store test data
retain_fact('bags.fm graduation pattern analysis shows 70% success rate', source='bags_intel')
retain_fact('Trading strategy: buy on graduation, sell at 2x', source='treasury')

# Test hybrid search
print('=== Hybrid Search Test ===')
results = hybrid_search('bags.fm graduation', limit=5)

print(f'Query: {results[\"query\"]}')
print(f'Results: {results[\"count\"]}')
print(f'Timing: FTS={results[\"timing\"][\"fts_ms\"]}ms, Vector={results[\"timing\"][\"vector_ms\"]}ms, Total={results[\"timing\"][\"total_ms\"]}ms')
print(f'Sources: FTS={results[\"sources\"][\"fts_count\"]}, Vector={results[\"sources\"][\"vector_count\"]}')

for r in results['results']:
    content = r['content'][:50] if r['content'] else 'N/A'
    print(f'  - RRF={r[\"rrf_score\"]:.4f} | {content}...')

print('SUCCESS')
"
```
  </verify>
  <done>
- rrf_fusion() merges FTS5 and vector results correctly
- hybrid_search() combines both search sources
- Graceful fallback to FTS5-only when PostgreSQL unavailable
- Timing breakdown shows performance of each component
  </done>
</task>

<task type="auto">
  <name>Task 3: Create unified recall interface</name>
  <files>core/memory/recall.py</files>
  <action>
Create recall.py with the primary recall() function.

```python
"""Unified recall interface for Jarvis memory system."""
from typing import Optional, List, Dict, Any, Literal

from .hybrid_search import hybrid_search
from .search import search_by_entity, get_recent_facts, get_entity_summary


TimeFilter = Literal["all", "today", "week", "month", "quarter", "year"]


def recall(
    query: str,
    k: int = 5,
    time_filter: TimeFilter = "all",
    source: Optional[str] = None,
    use_vector: bool = True,
) -> List[Dict[str, Any]]:
    """
    Query memory for relevant facts using hybrid search.

    This is the primary entry point for memory retrieval.
    Combines FTS5 keyword search with vector semantic search.

    Args:
        query: Natural language query (e.g., "bags.fm graduations that pumped").
        k: Number of results to return (default 5).
        time_filter: Temporal scope ('all', 'today', 'week', 'month', 'quarter', 'year').
        source: Filter by source system ('telegram', 'treasury', 'x', 'bags_intel').
        use_vector: Include semantic vector search (requires PostgreSQL).

    Returns:
        List of fact dicts with content, context, source, and relevance scores.

    Example:
        # Query similar past trades before entering position
        similar_trades = recall("bags.fm graduations", k=5)

        # Get recent user interactions
        user_context = recall("lucid preferences", k=3, time_filter="week")

        # Find treasury-specific patterns
        trade_patterns = recall("successful exits", k=10, source="treasury")
    """
    response = hybrid_search(
        query=query,
        limit=k,
        time_filter=time_filter,
        source=source,
        use_vector=use_vector,
    )

    # Return just the results list for simplicity
    return response.get("results", [])


def recall_with_metadata(
    query: str,
    k: int = 5,
    time_filter: TimeFilter = "all",
    source: Optional[str] = None,
    use_vector: bool = True,
) -> Dict[str, Any]:
    """
    Query memory with full response metadata.

    Same as recall() but returns timing and source information.

    Args:
        query: Search query.
        k: Number of results.
        time_filter: Temporal filter.
        source: Source filter.
        use_vector: Use vector search.

    Returns:
        Full response dict with results, count, timing, sources.
    """
    return hybrid_search(
        query=query,
        limit=k,
        time_filter=time_filter,
        source=source,
        use_vector=use_vector,
    )


def recall_entity(
    entity_name: str,
    k: int = 10,
    time_filter: TimeFilter = "all",
) -> Dict[str, Any]:
    """
    Get all facts about a specific entity.

    Args:
        entity_name: Entity name (e.g., '@KR8TIV', 'lucid').
        k: Maximum facts to return.
        time_filter: Temporal filter.

    Returns:
        Dict with entity summary and related facts.
    """
    # Get entity summary
    summary = get_entity_summary(entity_name)

    # Get related facts
    facts = search_by_entity(entity_name, limit=k, time_filter=time_filter)

    return {
        "entity": summary,
        "facts": facts,
        "count": len(facts),
    }


def recall_recent(
    k: int = 10,
    source: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """
    Get most recent facts.

    Args:
        k: Number of facts to return.
        source: Optional source filter.

    Returns:
        List of recent facts.
    """
    return get_recent_facts(limit=k, source=source)


def recall_context(
    topic: str,
    k: int = 5,
    include_recent: bool = True,
) -> Dict[str, Any]:
    """
    Build context for a topic by combining search and recent facts.

    Useful for preparing context before responding or making decisions.

    Args:
        topic: Topic to build context for.
        k: Results per source.
        include_recent: Include recent facts regardless of topic.

    Returns:
        Dict with topic_facts, recent_facts, and combined context.
    """
    # Search for topic-related facts
    topic_facts = recall(topic, k=k)

    # Get recent facts for broader context
    recent_facts = []
    if include_recent:
        recent_facts = recall_recent(k=k)

    # Deduplicate
    seen_ids = set()
    combined = []

    for fact in topic_facts:
        fact_id = fact.get("id")
        if fact_id and fact_id not in seen_ids:
            seen_ids.add(fact_id)
            combined.append(fact)

    for fact in recent_facts:
        fact_id = fact.get("id")
        if fact_id and fact_id not in seen_ids:
            seen_ids.add(fact_id)
            combined.append(fact)

    return {
        "topic": topic,
        "topic_facts": topic_facts,
        "recent_facts": recent_facts,
        "combined": combined,
        "total_count": len(combined),
    }
```

Update core/memory/__init__.py with all exports:

```python
"""Jarvis Memory System - Clawdbot-inspired dual-layer memory architecture."""
from .config import MemoryConfig, get_config
from .workspace import init_workspace, get_memory_path, MEMORY_ROOT
from .database import init_database, get_connection, get_db_manager, DatabaseManager
from .schema import SCHEMA_SQL, FTS5_SQL, TRIGGERS_SQL, INDEXES_SQL
from .retain import retain_fact, retain_preference, get_or_create_entity, get_user_preferences
from .markdown_sync import (
    append_to_daily_log,
    sync_fact_to_markdown,
    extract_entities_from_text,
    get_daily_log_path,
)
from .search import (
    search_facts,
    search_by_entity,
    search_by_source,
    get_recent_facts,
    get_entity_summary,
    get_facts_count,
)
from .postgres_integration import (
    is_postgres_available,
    check_archival_memory_exists,
    get_archival_memory_count,
    vector_search,
    store_embedding,
    generate_embedding,
)
from .hybrid_search import hybrid_search, rrf_fusion
from .recall import recall, recall_with_metadata, recall_entity, recall_recent, recall_context

__all__ = [
    # Config
    "MemoryConfig",
    "get_config",
    # Workspace
    "init_workspace",
    "get_memory_path",
    "MEMORY_ROOT",
    # Database
    "init_database",
    "get_connection",
    "get_db_manager",
    "DatabaseManager",
    # Schema
    "SCHEMA_SQL",
    "FTS5_SQL",
    "TRIGGERS_SQL",
    "INDEXES_SQL",
    # Retain
    "retain_fact",
    "retain_preference",
    "get_or_create_entity",
    "get_user_preferences",
    # Markdown
    "append_to_daily_log",
    "sync_fact_to_markdown",
    "extract_entities_from_text",
    "get_daily_log_path",
    # Search (FTS5)
    "search_facts",
    "search_by_entity",
    "search_by_source",
    "get_recent_facts",
    "get_entity_summary",
    "get_facts_count",
    # PostgreSQL
    "is_postgres_available",
    "check_archival_memory_exists",
    "get_archival_memory_count",
    "vector_search",
    "store_embedding",
    "generate_embedding",
    # Hybrid Search
    "hybrid_search",
    "rrf_fusion",
    # Recall (Primary Interface)
    "recall",
    "recall_with_metadata",
    "recall_entity",
    "recall_recent",
    "recall_context",
]
```
  </action>
  <verify>
```bash
python -c "
from core.memory import (
    init_workspace, init_database, retain_fact,
    recall, recall_with_metadata, recall_entity, recall_recent, recall_context
)

# Initialize
init_workspace()
init_database()

# Store sample facts
facts = [
    ('KR8TIV graduated from bags.fm with 85% bonding score', 'monitoring', 'bags_intel'),
    ('Bought KR8TIV at \$0.05 entry price', 'trade entry', 'treasury'),
    ('KR8TIV pumped to \$0.15, taking profit at 3x', 'trade exit', 'treasury'),
    ('User lucid prefers aggressive TP/SL strategy', 'preferences', 'telegram'),
]
for content, context, source in facts:
    retain_fact(content, context=context, source=source)

# Test recall
print('=== recall() Test ===')
results = recall('bags.fm graduation', k=3)
print(f'Found {len(results)} results')
for r in results[:2]:
    print(f'  - {r[\"content\"][:50]}...')

# Test recall_with_metadata
print('\n=== recall_with_metadata() Test ===')
response = recall_with_metadata('KR8TIV trading', k=5)
print(f'Results: {response[\"count\"]}, Total time: {response[\"timing\"][\"total_ms\"]}ms')

# Test recall_entity
print('\n=== recall_entity() Test ===')
entity_data = recall_entity('KR8TIV', k=3)
print(f'Entity: {entity_data[\"entity\"]}')
print(f'Related facts: {entity_data[\"count\"]}')

# Test recall_recent
print('\n=== recall_recent() Test ===')
recent = recall_recent(k=3)
print(f'Recent facts: {len(recent)}')

# Test recall_context
print('\n=== recall_context() Test ===')
context = recall_context('bags.fm trading', k=3)
print(f'Topic facts: {len(context[\"topic_facts\"])}')
print(f'Recent facts: {len(context[\"recent_facts\"])}')
print(f'Combined: {context[\"total_count\"]}')

print('\nSUCCESS - All recall functions working!')
"
```
  </verify>
  <done>
- recall() provides simple list interface
- recall_with_metadata() includes timing and source info
- recall_entity() returns entity summary + related facts
- recall_recent() returns chronological facts
- recall_context() builds combined context for topics
- All functions work with or without PostgreSQL (graceful degradation)
  </done>
</task>

</tasks>

<verification>
1. Test full memory flow (retain -> recall):
```bash
python -c "
from core.memory import init_workspace, init_database, retain_fact, recall

init_workspace()
init_database()

# Retain
fact_id = retain_fact(
    'Testing full memory flow with bags.fm token XYZ',
    context='integration test',
    source='system'
)
print(f'Retained fact: {fact_id}')

# Recall
results = recall('bags.fm token', k=3)
print(f'Recalled {len(results)} facts')
for r in results:
    print(f'  - {r[\"content\"][:40]}...')
"
```

2. Verify PostgreSQL integration (if available):
```bash
python -c "
from core.memory import is_postgres_available, get_archival_memory_count

if is_postgres_available():
    count = get_archival_memory_count()
    print(f'PostgreSQL archival_memory: {count} entries')
else:
    print('PostgreSQL not configured - using FTS5 only')
"
```

3. Performance check:
```bash
python -c "
from core.memory import init_workspace, init_database, recall_with_metadata

init_workspace()
init_database()

response = recall_with_metadata('test query', k=5)
print(f'Total latency: {response[\"timing\"][\"total_ms\"]}ms')
assert response['timing']['total_ms'] < 200, 'Too slow!'
print('Performance OK')
"
```
</verification>

<success_criteria>
- recall() returns ranked results from hybrid search
- RRF fusion correctly merges FTS5 and vector results
- PostgreSQL integration works when DATABASE_URL is set
- Graceful fallback to FTS5-only when PostgreSQL unavailable
- fact_embeddings table links SQLite facts to PostgreSQL IDs
- All recall variants work (recall, recall_entity, recall_recent, recall_context)
- Total recall latency <200ms (allows for embedding generation if needed)
- Existing 100+ archival_memory learnings accessible
</success_criteria>

<output>
After completion, create `.planning/memory-integration/phases/06-memory-foundation/06-05-SUMMARY.md`
</output>
