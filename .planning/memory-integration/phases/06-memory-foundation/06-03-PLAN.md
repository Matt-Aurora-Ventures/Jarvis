---
phase: 06-memory-foundation
plan: 03
type: execute
wave: 2
depends_on: ["06-01", "06-02"]
files_modified:
  - core/memory/retain.py
  - core/memory/markdown_sync.py
autonomous: true

must_haves:
  truths:
    - "retain_fact() stores facts in both SQLite and daily Markdown log"
    - "Daily log files are auto-created at memory/YYYY-MM-DD.md"
    - "Entity mentions are extracted and linked to facts"
    - "Markdown entries include timestamp, source, context, and entities"
    - "Entity extraction wiring verified: extract_entities -> get_or_create_entity -> entity_mentions"
  artifacts:
    - path: "core/memory/retain.py"
      provides: "Core retain_fact() function"
      exports: ["retain_fact", "retain_preference", "get_or_create_entity"]
    - path: "core/memory/markdown_sync.py"
      provides: "Markdown layer synchronization"
      exports: ["append_to_daily_log", "sync_fact_to_markdown"]
  key_links:
    - from: "core/memory/retain.py"
      to: "core/memory/database.py"
      via: "db.transaction()"
      pattern: "transaction.*INSERT INTO facts"
    - from: "core/memory/retain.py"
      to: "core/memory/markdown_sync.py"
      via: "sync_fact_to_markdown()"
      pattern: "sync_fact_to_markdown"
---

<!--
  Wave 2 parallel with Plan 04 (search.py):
  - Plan 03 (this): WRITES facts to database and Markdown
  - Plan 04: READS facts from database via FTS5
  These are independent - neither uses the other's code. Both depend only on
  database schema (Plan 02). Parallel execution is safe.
-->

<objective>
Implement the retain_fact() function that stores facts in both the SQLite database and daily Markdown logs, following the Clawdbot dual-layer architecture pattern.

Purpose: Enable all Jarvis bot systems to store facts with automatic entity extraction and linking. The dual-layer approach ensures both machine-queryable storage (SQLite) and human-readable logs (Markdown).

Output:
- core/memory/retain.py with retain_fact() and retain_preference() functions
- core/memory/markdown_sync.py with Markdown layer utilities
- Facts stored in both SQLite facts table and memory/YYYY-MM-DD.md
</objective>

<execution_context>
@C:\Users\lucid\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\lucid\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/memory-integration/PROJECT.md
@.planning/memory-integration/phases/06-memory-foundation/06-RESEARCH.md

From RESEARCH.md Pattern 1: Dual-Layer Sync (Markdown + SQLite):
```python
def retain_fact(content: str, context: str = None, entities: list[str] = None, source: str = None):
    """Store fact in both layers"""
    # 1. Insert into SQLite
    # 2. Append to daily Markdown log
    # 3. Update FTS5 index (automatic via trigger)
    # 4. Link entities via entity_mentions
```

Entity types from schema:
- 'token': @KR8TIV, @BONK, etc.
- 'user': @lucid, Telegram users
- 'strategy': Trading strategies
- 'platform': @bags.fm, @Jupiter, etc.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Markdown sync utilities</name>
  <files>core/memory/markdown_sync.py</files>
  <action>
Create markdown_sync.py with functions to sync facts to daily logs.

```python
"""Markdown layer synchronization for dual-layer memory architecture."""
import re
from datetime import datetime
from pathlib import Path
from typing import Optional, List

from .config import get_config


def get_daily_log_path(date: Optional[datetime] = None) -> Path:
    """
    Get path to daily log file.

    Args:
        date: Date for log. Defaults to UTC now.

    Returns:
        Path to daily log Markdown file.
    """
    if date is None:
        date = datetime.utcnow()

    config = get_config()
    filename = date.strftime("%Y-%m-%d.md")
    return config.daily_logs_dir / filename


def ensure_daily_log_exists(date: Optional[datetime] = None) -> Path:
    """
    Ensure daily log file exists, creating with header if needed.

    Args:
        date: Date for log. Defaults to UTC now.

    Returns:
        Path to daily log file.
    """
    if date is None:
        date = datetime.utcnow()

    log_path = get_daily_log_path(date)

    # Ensure directory exists
    log_path.parent.mkdir(parents=True, exist_ok=True)

    if not log_path.exists():
        header = f"""# Daily Log: {date.strftime("%Y-%m-%d")}

*Memory entries for {date.strftime("%A, %B %d, %Y")}*

---

"""
        log_path.write_text(header, encoding="utf-8")

    return log_path


def format_fact_entry(
    content: str,
    context: Optional[str] = None,
    source: Optional[str] = None,
    entities: Optional[List[str]] = None,
    confidence: float = 1.0,
    timestamp: Optional[datetime] = None,
) -> str:
    """
    Format a fact as a Markdown entry.

    Args:
        content: The fact content.
        context: Optional context/situation.
        source: Source system (telegram, treasury, etc.).
        entities: List of entity mentions.
        confidence: Confidence score (0.0-1.0).
        timestamp: Entry timestamp. Defaults to now.

    Returns:
        Formatted Markdown string.
    """
    if timestamp is None:
        timestamp = datetime.utcnow()

    lines = []

    # Header with timestamp
    lines.append(f"## {timestamp.strftime('%H:%M:%S UTC')}")
    lines.append("")

    # Content
    lines.append(content)
    lines.append("")

    # Metadata
    metadata_parts = []

    if source:
        metadata_parts.append(f"**Source:** {source}")

    if context:
        metadata_parts.append(f"**Context:** {context}")

    if entities:
        entity_str = ", ".join(f"`{e}`" for e in entities)
        metadata_parts.append(f"**Entities:** {entity_str}")

    if confidence < 1.0:
        metadata_parts.append(f"**Confidence:** {confidence:.2f}")

    if metadata_parts:
        lines.append(" | ".join(metadata_parts))
        lines.append("")

    lines.append("---")
    lines.append("")

    return "\n".join(lines)


def append_to_daily_log(
    content: str,
    context: Optional[str] = None,
    source: Optional[str] = None,
    entities: Optional[List[str]] = None,
    confidence: float = 1.0,
    date: Optional[datetime] = None,
) -> Path:
    """
    Append a fact to the daily log file.

    Creates the log file with header if it doesn't exist.

    Args:
        content: The fact content.
        context: Optional context.
        source: Source system.
        entities: Entity mentions.
        confidence: Confidence score.
        date: Date for log. Defaults to today.

    Returns:
        Path to the log file.
    """
    log_path = ensure_daily_log_exists(date)

    entry = format_fact_entry(
        content=content,
        context=context,
        source=source,
        entities=entities,
        confidence=confidence,
        timestamp=date or datetime.utcnow(),
    )

    # Append to file
    with open(log_path, "a", encoding="utf-8") as f:
        f.write(entry)

    return log_path


def sync_fact_to_markdown(
    fact_id: int,
    content: str,
    context: Optional[str] = None,
    source: Optional[str] = None,
    entities: Optional[List[str]] = None,
    confidence: float = 1.0,
    timestamp: Optional[datetime] = None,
) -> Path:
    """
    Sync a stored fact to Markdown (for use after SQLite insert).

    This is the primary function called by retain_fact().

    Args:
        fact_id: SQLite fact ID (for reference).
        content: Fact content.
        context: Optional context.
        source: Source system.
        entities: Entity mentions.
        confidence: Confidence score.
        timestamp: Fact timestamp.

    Returns:
        Path to the log file where fact was written.
    """
    return append_to_daily_log(
        content=content,
        context=context,
        source=source,
        entities=entities,
        confidence=confidence,
        date=timestamp,
    )


def extract_entities_from_text(text: str) -> List[str]:
    """
    Extract entity mentions from text.

    Patterns:
    - @mentions (Twitter-style)
    - Token symbols (uppercase 3-6 chars)
    - Known platforms (bags.fm, Jupiter, etc.)

    Args:
        text: Text to extract entities from.

    Returns:
        List of entity names.
    """
    entities = set()

    # 1. @mentions
    mentions = re.findall(r"@(\w+)", text)
    entities.update(f"@{m}" for m in mentions)

    # 2. Token symbols (uppercase, 3-6 chars, not common words)
    common_words = {"THE", "AND", "FOR", "NOT", "BUT", "USD", "SOL", "ETH", "BTC"}
    tokens = re.findall(r"\b([A-Z]{3,6})\b", text)
    for token in tokens:
        if token not in common_words:
            entities.add(token)

    # 3. Known platforms
    platforms = ["bags.fm", "Jupiter", "Raydium", "Orca", "Telegram", "Twitter", "X"]
    text_lower = text.lower()
    for platform in platforms:
        if platform.lower() in text_lower:
            entities.add(platform)

    return list(entities)
```
  </action>
  <verify>
```bash
python -c "
from core.memory.markdown_sync import (
    get_daily_log_path,
    ensure_daily_log_exists,
    format_fact_entry,
    extract_entities_from_text
)

# Test entity extraction
text = 'Bought @KR8TIV after bags.fm graduation. BONK also pumping.'
entities = extract_entities_from_text(text)
print(f'Entities extracted: {entities}')

# Test formatting
entry = format_fact_entry(
    content=text,
    context='bags.fm monitoring',
    source='treasury',
    entities=entities
)
print('Formatted entry:')
print(entry[:200])

# Test log path
path = get_daily_log_path()
print(f'Daily log path: {path}')

print('SUCCESS')
"
```
  </verify>
  <done>
- get_daily_log_path() returns correct path for date
- ensure_daily_log_exists() creates log with header
- format_fact_entry() produces clean Markdown
- extract_entities_from_text() finds @mentions, tokens, platforms
  </done>
</task>

<task type="auto">
  <name>Task 2: Create retain functions</name>
  <files>core/memory/retain.py</files>
  <action>
Create retain.py with the core retain_fact() and retain_preference() functions.

```python
"""Retain functions for storing facts and preferences in memory."""
import sqlite3
from datetime import datetime
from typing import Optional, List, Dict, Any

from .database import get_db_manager
from .markdown_sync import sync_fact_to_markdown, extract_entities_from_text


def get_or_create_entity(
    conn: sqlite3.Connection,
    name: str,
    entity_type: Optional[str] = None,
) -> int:
    """
    Get existing entity ID or create new entity.

    Args:
        conn: Database connection.
        name: Entity name (e.g., '@KR8TIV', 'lucid').
        entity_type: Type hint ('token', 'user', 'strategy', 'platform').

    Returns:
        Entity ID.
    """
    # Try to get existing entity
    result = conn.execute(
        "SELECT id, type FROM entities WHERE name = ?",
        (name,)
    ).fetchone()

    if result:
        return result["id"]

    # Infer entity type if not provided
    if entity_type is None:
        entity_type = _infer_entity_type(name)

    # Create new entity
    cursor = conn.execute(
        """
        INSERT INTO entities (name, type, created_at, last_updated)
        VALUES (?, ?, ?, ?)
        """,
        (name, entity_type, datetime.utcnow(), datetime.utcnow())
    )

    return cursor.lastrowid


def _infer_entity_type(name: str) -> str:
    """
    Infer entity type from name patterns.

    Args:
        name: Entity name.

    Returns:
        Inferred type string.
    """
    name_lower = name.lower()

    # @mentions are usually users
    if name.startswith("@"):
        # But some @mentions are tokens
        inner = name[1:]
        if inner.isupper() and len(inner) <= 6:
            return "token"
        return "user"

    # All uppercase 3-6 chars = token
    if name.isupper() and 3 <= len(name) <= 6:
        return "token"

    # Known platforms
    platforms = ["bags.fm", "jupiter", "raydium", "orca", "telegram", "twitter"]
    if name_lower in platforms:
        return "platform"

    # Default
    return "other"


def retain_fact(
    content: str,
    context: Optional[str] = None,
    entities: Optional[List[str]] = None,
    source: Optional[str] = None,
    confidence: float = 1.0,
    auto_extract_entities: bool = True,
) -> int:
    """
    Store a fact in both SQLite and daily Markdown log.

    This is the primary entry point for storing memory.

    Args:
        content: The fact content (required).
        context: Situational context (e.g., 'bags.fm graduation').
        entities: Explicit entity mentions. If None and auto_extract_entities=True,
                  entities are extracted from content.
        source: Source system ('telegram', 'treasury', 'x', 'bags_intel', 'buy_tracker').
        confidence: Confidence score 0.0-1.0 (default 1.0).
        auto_extract_entities: Auto-extract entities from content if none provided.

    Returns:
        The fact ID.

    Example:
        fact_id = retain_fact(
            content="KR8TIV bought at $0.05, sold at $0.12 (+140%)",
            context="bags.fm graduation within 2h, sentiment: 0.85",
            entities=["@KR8TIV", "@bags.fm", "@lucid"],
            source="treasury"
        )
    """
    db = get_db_manager()
    timestamp = datetime.utcnow()

    # Auto-extract entities if not provided
    if entities is None and auto_extract_entities:
        entities = extract_entities_from_text(content)
        if context:
            entities.extend(extract_entities_from_text(context))
        entities = list(set(entities))  # Dedupe

    with db.transaction() as conn:
        # 1. Insert into facts table
        cursor = conn.execute(
            """
            INSERT INTO facts (content, context, timestamp, source, confidence)
            VALUES (?, ?, ?, ?, ?)
            """,
            (content, context, timestamp, source, confidence)
        )
        fact_id = cursor.lastrowid

        # 2. Link entities
        if entities:
            for entity_name in entities:
                entity_id = get_or_create_entity(conn, entity_name)
                try:
                    conn.execute(
                        """
                        INSERT INTO entity_mentions (fact_id, entity_id)
                        VALUES (?, ?)
                        """,
                        (fact_id, entity_id)
                    )
                except sqlite3.IntegrityError:
                    # Duplicate mention, skip
                    pass

    # 3. Sync to Markdown (outside transaction - file I/O)
    sync_fact_to_markdown(
        fact_id=fact_id,
        content=content,
        context=context,
        source=source,
        entities=entities,
        confidence=confidence,
        timestamp=timestamp,
    )

    return fact_id


def retain_preference(
    user: str,
    key: str,
    value: str,
    evidence: Optional[str] = None,
    confirmed: bool = True,
) -> Dict[str, Any]:
    """
    Store or update a user preference with confidence evolution.

    Confidence increases with confirmations, decreases with contradictions.

    Args:
        user: User identifier (e.g., 'lucid').
        key: Preference key (e.g., 'risk_tolerance').
        value: Preference value.
        evidence: Evidence for this preference update.
        confirmed: True if this confirms the preference, False if contradicts.

    Returns:
        Dict with preference state: {id, confidence, evidence_count}.

    Example:
        pref = retain_preference(
            user="lucid",
            key="risk_tolerance",
            value="aggressive",
            evidence="User said 'I want max gains'",
            confirmed=True
        )
    """
    db = get_db_manager()
    timestamp = datetime.utcnow()

    with db.transaction() as conn:
        # Get current preference state
        current = conn.execute(
            "SELECT id, confidence, evidence_count FROM preferences WHERE user = ? AND key = ?",
            (user, key)
        ).fetchone()

        if current:
            pref_id = current["id"]
            old_confidence = current["confidence"]
            evidence_count = current["evidence_count"]

            # Confidence evolution:
            # - Confirmation: +0.1 (max 0.95)
            # - Contradiction: -0.15 (min 0.1)
            if confirmed:
                new_confidence = min(0.95, old_confidence + 0.1)
            else:
                new_confidence = max(0.1, old_confidence - 0.15)

            # Update preference
            conn.execute(
                """
                UPDATE preferences
                SET value = ?, confidence = ?, evidence_count = ?, last_updated = ?
                WHERE id = ?
                """,
                (value, new_confidence, evidence_count + 1, timestamp, pref_id)
            )

            result = {
                "id": pref_id,
                "confidence": new_confidence,
                "evidence_count": evidence_count + 1,
                "updated": True,
            }
        else:
            # Create new preference (start at 0.5 confidence)
            cursor = conn.execute(
                """
                INSERT INTO preferences (user, key, value, confidence, evidence_count, created_at, last_updated)
                VALUES (?, ?, ?, 0.5, 1, ?, ?)
                """,
                (user, key, value, timestamp, timestamp)
            )

            result = {
                "id": cursor.lastrowid,
                "confidence": 0.5,
                "evidence_count": 1,
                "updated": False,
            }

        # Store evidence as fact
        if evidence:
            action = "confirmed" if confirmed else "contradicted"
            retain_fact(
                content=f"User {user} preference: {key}={value} ({action})",
                context=evidence,
                source="preference_tracking",
                entities=[f"@{user}"],
                confidence=result["confidence"],
            )

    return result


def get_user_preferences(user: str) -> List[Dict[str, Any]]:
    """
    Get all preferences for a user.

    Args:
        user: User identifier.

    Returns:
        List of preference dicts with key, value, confidence, evidence_count.
    """
    db = get_db_manager()

    with db.connection() as conn:
        rows = conn.execute(
            """
            SELECT key, value, confidence, evidence_count, last_updated
            FROM preferences
            WHERE user = ?
            ORDER BY last_updated DESC
            """,
            (user,)
        ).fetchall()

        return [
            {
                "key": row["key"],
                "value": row["value"],
                "confidence": row["confidence"],
                "evidence_count": row["evidence_count"],
                "last_updated": row["last_updated"],
            }
            for row in rows
        ]
```

Update core/memory/__init__.py to export retain functions:

```python
"""Jarvis Memory System - Clawdbot-inspired dual-layer memory architecture."""
from .config import MemoryConfig, get_config
from .workspace import init_workspace, get_memory_path, MEMORY_ROOT
from .database import init_database, get_connection, get_db_manager, DatabaseManager
from .schema import SCHEMA_SQL, FTS5_SQL, TRIGGERS_SQL, INDEXES_SQL
from .retain import retain_fact, retain_preference, get_or_create_entity, get_user_preferences
from .markdown_sync import (
    append_to_daily_log,
    sync_fact_to_markdown,
    extract_entities_from_text,
    get_daily_log_path,
)

__all__ = [
    # Config
    "MemoryConfig",
    "get_config",
    # Workspace
    "init_workspace",
    "get_memory_path",
    "MEMORY_ROOT",
    # Database
    "init_database",
    "get_connection",
    "get_db_manager",
    "DatabaseManager",
    # Schema
    "SCHEMA_SQL",
    "FTS5_SQL",
    "TRIGGERS_SQL",
    "INDEXES_SQL",
    # Retain
    "retain_fact",
    "retain_preference",
    "get_or_create_entity",
    "get_user_preferences",
    # Markdown
    "append_to_daily_log",
    "sync_fact_to_markdown",
    "extract_entities_from_text",
    "get_daily_log_path",
]
```
  </action>
  <verify>
```bash
python -c "
from core.memory import init_workspace, init_database, retain_fact, retain_preference, get_user_preferences
from datetime import datetime

# Initialize
init_workspace()
init_database()

# Test retain_fact
fact_id = retain_fact(
    content='Bought KR8TIV at \$0.05 after bags.fm graduation',
    context='Quick graduation, strong bonding curve',
    source='treasury'
)
print(f'Stored fact ID: {fact_id}')

# Verify in database
from core.memory import get_db_manager
db = get_db_manager()
with db.connection() as conn:
    fact = conn.execute('SELECT * FROM facts WHERE id = ?', (fact_id,)).fetchone()
    print(f'Fact in DB: {fact[\"content\"][:50]}...')

    entities = conn.execute('''
        SELECT e.name FROM entities e
        JOIN entity_mentions em ON e.id = em.entity_id
        WHERE em.fact_id = ?
    ''', (fact_id,)).fetchall()
    print(f'Linked entities: {[e[\"name\"] for e in entities]}')

# Test retain_preference
pref = retain_preference(
    user='lucid',
    key='risk_tolerance',
    value='aggressive',
    evidence='User said max gains',
    confirmed=True
)
print(f'Preference: confidence={pref[\"confidence\"]}, count={pref[\"evidence_count\"]}')

# Get all preferences
prefs = get_user_preferences('lucid')
print(f'User prefs: {len(prefs)} stored')

# Verify daily log exists
from core.memory import get_daily_log_path
log_path = get_daily_log_path()
print(f'Daily log exists: {log_path.exists()}')
if log_path.exists():
    content = log_path.read_text()[:200]
    print(f'Log preview: {content}')

print('SUCCESS')
"
```
  </verify>
  <done>
- retain_fact() stores facts in SQLite facts table
- retain_fact() auto-syncs to daily Markdown log
- retain_fact() auto-extracts entities from content
- Entity mentions are linked via entity_mentions table (VERIFIED: entity records created via get_or_create_entity(), links stored in entity_mentions)
- retain_preference() creates/updates preferences with confidence evolution
- get_user_preferences() retrieves all prefs for a user
  </done>
</task>

<task type="auto">
  <name>Task 3: Verify entity extraction to entity_mentions wiring</name>
  <files>core/memory/retain.py</files>
  <action>
Verify that extract_entities_from_text() properly feeds into entity_mentions table:

1. Confirm entity extraction produces expected entities
2. Confirm get_or_create_entity() creates entity records in entities table
3. Confirm entity_mentions records link fact_id to entity_id
4. This is a verification task - fix any gaps in the wiring

Verification script:
```python
from core.memory import init_workspace, init_database, retain_fact, get_db_manager

# Initialize
init_workspace()
init_database()

# Store fact with known entities
fact_id = retain_fact(
    content='Testing @KR8TIV token on bags.fm platform',
    context='entity extraction test',
    source='system'
)

# Verify entity extraction worked
db = get_db_manager()
with db.connection() as conn:
    # Check entities table has entries
    entities = conn.execute('''
        SELECT id, name, type FROM entities
        WHERE name IN ('@KR8TIV', 'KR8TIV', 'bags.fm')
    ''').fetchall()
    assert len(entities) > 0, "No entities created!"

    # Check entity_mentions links exist
    mentions = conn.execute('''
        SELECT em.fact_id, em.entity_id, e.name
        FROM entity_mentions em
        JOIN entities e ON em.entity_id = e.id
        WHERE em.fact_id = ?
    ''', (fact_id,)).fetchall()
    assert len(mentions) > 0, "No entity mentions linked!"

print(f"VERIFIED: {len(entities)} entities created, {len(mentions)} mentions linked")
```
  </action>
  <verify>
```bash
python -c "
from core.memory import init_workspace, init_database, retain_fact, get_db_manager

init_workspace()
init_database()

# Test entity extraction wiring
fact_id = retain_fact('Testing @KR8TIV on bags.fm', source='system')

db = get_db_manager()
with db.connection() as conn:
    entities = conn.execute('SELECT COUNT(*) as cnt FROM entities').fetchone()['cnt']
    mentions = conn.execute('SELECT COUNT(*) as cnt FROM entity_mentions WHERE fact_id = ?', (fact_id,)).fetchone()['cnt']
    print(f'Entities in DB: {entities}')
    print(f'Mentions for fact {fact_id}: {mentions}')
    assert mentions > 0, 'Entity extraction to entity_mentions wiring broken!'

print('ENTITY EXTRACTION WIRING VERIFIED')
"
```
  </verify>
  <done>
- extract_entities_from_text() returns list of entity names from content
- get_or_create_entity() creates or retrieves entity records
- entity_mentions table populated with (fact_id, entity_id) pairs
- Wiring from retain_fact() -> extract_entities -> get_or_create_entity -> entity_mentions verified
  </done>
</task>

</tasks>

<verification>
1. Test full retain flow:
```bash
python -c "
from core.memory import init_workspace, init_database, retain_fact

init_workspace()
init_database()

# Store multiple facts
facts = [
    ('Bought BONK at \$0.0001', 'meme season', 'treasury'),
    ('@lucid requested aggressive TP/SL settings', 'telegram command', 'telegram'),
    ('bags.fm graduation detected for XYZ token', 'monitoring', 'bags_intel'),
]

for content, context, source in facts:
    fact_id = retain_fact(content, context=context, source=source)
    print(f'Stored: {fact_id}')
"
```

2. Verify Markdown log has all entries:
```bash
cat ~/.lifeos/memory/memory/$(date +%Y-%m-%d).md
```

3. Verify FTS5 has entries:
```bash
python -c "
from core.memory import get_db_manager
db = get_db_manager()
with db.connection() as conn:
    results = conn.execute(\"SELECT * FROM facts_fts WHERE facts_fts MATCH 'bags.fm'\").fetchall()
    print(f'FTS5 found: {len(results)} matches')
"
```
</verification>

<success_criteria>
- retain_fact() returns valid fact ID
- Facts appear in SQLite facts table with all fields populated
- FTS5 index is auto-updated (trigger fires)
- Daily log file created at memory/YYYY-MM-DD.md
- Markdown entries are properly formatted with metadata
- Entities auto-extracted from content and context
- Entity mentions linked in entity_mentions table
- retain_preference() creates preferences with 0.5 starting confidence
- Confidence evolves: +0.1 for confirm, -0.15 for contradict
- get_user_preferences() returns all prefs for user
</success_criteria>

<output>
After completion, create `.planning/memory-integration/phases/06-memory-foundation/06-03-SUMMARY.md`
</output>
